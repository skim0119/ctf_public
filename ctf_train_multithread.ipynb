{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Notes\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "- This notebook does not include running the CtF game with the RL policy. Using the network will be separately scripted in policy/policy_RL1.py.\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='NO_RED_02_MULTI:'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.33 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# the modules that you can use to generate the policy.\n",
    "import policy.patrol \n",
    "import policy.random\n",
    "import policy.simple # custon written policy\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from DataModule import one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"cap-v0\") # initialize the environment\n",
    "policy_red = policy.random.PolicyGen(env.get_map, env.get_team_red)\n",
    "\n",
    "#UNKNOWN  = CONST.UNKNOWN # -1\n",
    "#TEAM1_BG = CONST.TEAM1_BACKGROUND # 0\n",
    "#TEAM2_BG = CONST.TEAM2_BACKGROUND # 1\n",
    "#TEAM1_AG = CONST.TEAM1_UGV # 2\n",
    "#TEAM2_AG = CONST.TEAM2_UGV # 4\n",
    "#TEAM1_FL = CONST.TEAM1_FLAG # 6\n",
    "#TEAM2_FL = CONST.TEAM2_FLAG # 7\n",
    "#OBSTACLE = CONST.OBSTACLE # 8\n",
    "#DEAD     = CONST.DEAD # 9\n",
    "#SELECTED = CONST.SELECTED # 10\n",
    "#COMPLETED= CONST.COMPLETED # 11\n",
    "\n",
    "VISION_RANGE = 10 #CONST.UGV_RANGE\n",
    "#VISION_dX    = 2*VISION_RANGE+1\n",
    "#VISION_dY    = 2*VISION_RANGE+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red number :  0\n",
      "blue number :  4\n",
      "vision range :  10\n"
     ]
    }
   ],
   "source": [
    "print('red number : ', len(env.get_team_red))\n",
    "print('blue number : ', len(env.get_team_blue))\n",
    "print('vision range : ', VISION_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "def discount_rewards(r, pre_discount=0):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0.0\n",
    "    pre_disc_factor = gamma**pre_discount\n",
    "    for t in reversed(range(r.size)):\n",
    "        running_add = (running_add * gamma + r[t]) * pre_disc_factor\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r = (discounted_r - np.mean(discounted_r)) / (np.std(discounted_r)+1e-8) # normalize\n",
    "    return discounted_r\n",
    "\n",
    "def discount_rewards_multiagent(r, n_agents):\n",
    "    discounted_r = np.reshape(r,(-1,n_agents)) # Reshape the r into vertical matrix with 'n_agents' columns\n",
    "    for idx in range(n_agents):\n",
    "        column = discounted_r[:,idx]                     # extract single column\n",
    "        discounted_r[:,idx] = discount_rewards(column);  # Run discount_reward on the column, and substitute\n",
    "    return np.reshape(discounted_r,(-1))                 # return the flattened matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, in_size,action_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_input = tf.placeholder(shape=in_size,dtype=tf.float32, name='state')\n",
    "        \n",
    "        layer = slim.conv2d(self.state_input, 16, [5,5], activation_fn=tf.nn.relu,\n",
    "                            padding='SAME',\n",
    "                            scope='conv1')\n",
    "        #layer = slim.avg_pool2d(layer, [2,2])\n",
    "        layer = slim.conv2d(layer, 16, [3,3], activation_fn=tf.nn.relu,\n",
    "                            padding='SAME',\n",
    "                            scope='conv2')\n",
    "        layer = slim.flatten(layer)\n",
    "        #flat  = tf.reshape(conv, [-1, VISION_dX*VISION_dY*128])\n",
    "        #layer = slim.dropout(layer,keep_prob=0.8)\n",
    "        #layer = slim.fully_connected(layer, 516,\n",
    "        #                            activation_fn=tf.nn.relu,\n",
    "        #                            biases_initializer=None,\n",
    "        #                            scope='hidden_fc1')\n",
    "        layer = slim.dropout(layer,keep_prob=0.8)\n",
    "        #layer = slim.fully_connected(layer, 516,\n",
    "        #                            #activation_fn=tf.nn.relu,\n",
    "        #                            biases_initializer=None)\n",
    "        self.dense = slim.fully_connected(layer, action_size,\n",
    "                                     activation_fn=tf.nn.relu,\n",
    "                                     biases_initializer=None,\n",
    "                                     scope='output_fc')\n",
    "        self.output = tf.nn.softmax(self.dense, name='action')\n",
    "        \n",
    "        tf.summary.histogram('output', self.output)\n",
    "        \n",
    "        with tf.name_scope('weights'):\n",
    "            for var in slim.get_model_variables():\n",
    "                tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "\n",
    "        with tf.name_scope('Records'):\n",
    "            self.mean_reward = tf.placeholder(\"float\", None)\n",
    "            self.mean_length = tf.placeholder(\"float\", None)\n",
    "            self.mean_succeed = tf.placeholder(\"float\", None)\n",
    "            #self.mean_stdaction = tf.placeholder(\"float\",None)\n",
    "            tf.summary.scalar('mean_reward', self.mean_reward)\n",
    "            tf.summary.scalar('mean_length', self.mean_length)\n",
    "            tf.summary.scalar('mean_succeed', self.mean_succeed)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            self.action_holder = tf.placeholder(tf.int32, [None, action_size])\n",
    "            self.reward_holder = tf.placeholder(tf.float32, [None])\n",
    "            self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.dense, labels = self.action_holder)\n",
    "            self.loss = -tf.reduce_mean(self.neg_log_prob * self.reward_holder) \n",
    "\n",
    "        with tf.name_scope('optimizer'):\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            #self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n",
    "            self.update_batch = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "myAgent = agent(lr=1e-5,in_size=[None,VISION_dX,VISION_dY,6],action_size=5) #Load the agent.\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step') # global step\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10000 #Set total number of episodes to train agent on.\n",
    "max_ep = 150\n",
    "update_frequency = 10\n",
    "save_network_frequency = 100\n",
    "save_reward_frequency = 50\n",
    "batch_size = 2000\n",
    "\n",
    "action_space = 5\n",
    "n_agent = len(env.get_team_blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def one_hot_encoder(state, agents):\n",
    "    ret = np.zeros((len(agents),VISION_dX,VISION_dY,6))\n",
    "    # team 1 : (1), team 2 : (-1), map elements: (0)\n",
    "    map_channel = {UNKNOWN:0, DEAD:0,\n",
    "                   TEAM1_BG:1, TEAM2_BG:1,\n",
    "                   TEAM1_AG:2, TEAM2_AG:2,\n",
    "                   3:3, 5:3, # UAV, does not need to be included for now\n",
    "                   TEAM1_FL:4, TEAM2_FL:4,\n",
    "                   OBSTACLE:5}\n",
    "    map_color   = {UNKNOWN:1, DEAD:0, OBSTACLE:1,\n",
    "                   TEAM1_BG:1, TEAM2_BG:-1,\n",
    "                   TEAM1_AG:1, TEAM2_AG:-1,\n",
    "                   3:1, 5:-1, # UAV, does not need to be included for now\n",
    "                   TEAM1_FL:1, TEAM2_FL:-1}\n",
    "    \n",
    "    # Expand the observation with 3-thickness wall\n",
    "    # - in order to avoid dealing with the boundary\n",
    "    sx, sy = state.shape\n",
    "    _state = np.ones((sx+2*VISION_RANGE, sy+2*VISION_RANGE)) * OBSTACLE # 8 for obstacle\n",
    "    _state[VISION_RANGE:VISION_RANGE+sx, VISION_RANGE:VISION_RANGE+sy] = state\n",
    "    state = _state\n",
    "\n",
    "    for idx,agent in enumerate(agents):\n",
    "        # Initialize Variables\n",
    "        x, y = agent.get_loc()\n",
    "        x += VISION_RANGE\n",
    "        y += VISION_RANGE\n",
    "        vision = state[x-VISION_RANGE:x+VISION_RANGE+1,y-VISION_RANGE:y+VISION_RANGE+1] # extract the limited view for the agent (5x5)\n",
    "        for i in range(len(vision)):\n",
    "            for j in range(len(vision[0])):\n",
    "                if vision[i][j] != -1:\n",
    "                    channel = map_channel[vision[i][j]]\n",
    "                    ret[idx][i][j][channel] = map_color[vision[i][j]]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "# Launch the session\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#sess = tf.Session()\n",
    "\n",
    "total_reward = []\n",
    "total_length = []\n",
    "total_captured = []\n",
    "\n",
    "# Setup Save and Restore Network\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(obs):\n",
    "    feed_dict = {myAgent.mean_reward:np.mean(total_reward[-save_reward_frequency:]),\n",
    "                         myAgent.mean_length:np.mean(total_length[-save_reward_frequency:]),\n",
    "                         myAgent.mean_succeed:np.mean(total_captured[-save_reward_frequency:]),\n",
    "                         myAgent.state_input:obs\n",
    "                }\n",
    "    with tf.device('/cpu:0'):    \n",
    "        summary_str = sess.run(merged, feed_dict=feed_dict)\n",
    "        writer.add_summary(summary_str, sess.run(global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size=2000):\n",
    "        if size > len(self.buffer):\n",
    "            return np.array(self.buffer)\n",
    "        else:\n",
    "            return np.reshape(np.array(random.sample(self.buffer,size)),[size,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_rollout(env, explore=False):\n",
    "    # Run single episode, return the results (number of frame, history, total reward, game_won, last obs)\n",
    "    s = env.reset(map_size=20, policy_red=policy_red)\n",
    "    ep_history = []\n",
    "    indv_history = [[] for _ in range(len(env.get_team_blue))]\n",
    "    \n",
    "    was_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "    prev_reward=0\n",
    "    for frame in range(max_ep+1):\n",
    "        #obs = one_hot_encoder(s, env.get_team_blue).tolist() # partial observation\n",
    "        obs = one_hot_encoder(env._env, env.get_team_blue) # Full observation\n",
    "        \n",
    "        if explore:\n",
    "            act = np.random.randint(action_space, size=n_agent).tolist()\n",
    "        else:\n",
    "            with tf.device('/cpu:0'):\n",
    "                #Probabilistically pick an action given our network outputs.\n",
    "                act_prob = sess.run(myAgent.output, feed_dict={myAgent.state_input:obs})\n",
    "                act = [np.random.choice(action_space, p=act_prob[x]/sum(act_prob[x])) for x in range(n_agent)] # divide by sum : normalize\n",
    "            \n",
    "        s,r,d,_ = env.step(act) #Get our reward for taking an action given a bandit.\n",
    "        \n",
    "        # If frame is at max and the game is still not done, negative reward\n",
    "        if frame == max_ep and d == False:\n",
    "            r -= frame * (30/150)\n",
    "            #r -= 30\n",
    "            \n",
    "        # Push history for individual that 'was' alive previous frame\n",
    "        for idx, agent in enumerate(env.get_team_blue):\n",
    "            if was_alive[idx]: indv_history[idx].append([obs[idx],act[idx],r])\n",
    "\n",
    "        # If reward sequence change, push the history into the episode history.\n",
    "        if prev_reward != r:\n",
    "            for idx, history in enumerate(indv_history):\n",
    "                if len(history)==0: continue\n",
    "                if not was_alive[idx]: continue\n",
    "                _history = np.array(history)\n",
    "                _history[:,2] = discount_rewards(_history[:,2])\n",
    "                ep_history.extend(_history)\n",
    "        \n",
    "        # State Transition\n",
    "        frame += 1\n",
    "        prev_reward = r\n",
    "        was_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "        \n",
    "        if d == True:\n",
    "            break\n",
    "    if len(ep_history) > 0:        \n",
    "        ep_history = np.stack(ep_history)\n",
    "    \n",
    "    return [frame, ep_history, r, env.game_won, obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5/10000 [.....] - ETA: 5:01:33\n",
      "\n",
      "Manually stopped the training (KeyboardInterrupt)\n",
      "save:  5 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = 0\n",
    "\n",
    "exploration_count = 20\n",
    "exp_buffer = Experience_buffer()\n",
    "try:\n",
    "    progbar = tf.keras.utils.Progbar(total_episodes,width=5)\n",
    "    while ep < total_episodes+1:\n",
    "        progbar.update(ep) # update progress bar\n",
    "         \n",
    "        # Run episode\n",
    "        frame, history, reward, did_won, obs = policy_rollout(env, exploration_count > ep)\n",
    "        \n",
    "        # Add history\n",
    "        exp_buffer.add(history)\n",
    "\n",
    "        if exploration_count < ep:\n",
    "            if ep % update_frequency == 0 and ep != 0:\n",
    "                with tf.device('/gpu:0'):\n",
    "                    batch_history = exp_buffer.sample(batch_size) # Sample from experience replay\n",
    "                    feed_dict = {myAgent.state_input: np.stack(batch_history[:,0]),\n",
    "                                                myAgent.action_holder:sess.run(tf.one_hot(batch_history[:,1],5)),\n",
    "                                                myAgent.reward_holder: batch_history[:,2]}\n",
    "                    loss_, _ = sess.run([myAgent.loss, myAgent.update_batch],\n",
    "                                        feed_dict=feed_dict)\n",
    "\n",
    "        total_reward.append(reward)\n",
    "        total_length.append(frame)\n",
    "        total_captured.append(env.game_won)\n",
    "        if ep % save_reward_frequency == 0 and ep != 0:\n",
    "            record(obs)\n",
    "        \n",
    "        # save every 100 ep\n",
    "        if ep % save_network_frequency == 0 and ep != 0:\n",
    "            print(\" Training Loss: {}\".format(loss_))\n",
    "            print(' Average r : ', np.mean(total_reward[-save_network_frequency:]))\n",
    "            saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_step)\n",
    "            print(\"save weights: \", sess.run(global_step), 'episodes')\n",
    "\n",
    "        ep += 1\n",
    "        sess.run(increment_global_step_op)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('\\n\\nManually stopped the training (KeyboardInterrupt)');\n",
    "    plt.plot(total_reward)\n",
    "    plt.figure()\n",
    "    plt.plot(total_length)\n",
    "    plt.figure()\n",
    "    plt.plot(total_captured)\n",
    "    saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_step)\n",
    "    record(obs)\n",
    "    print(\"save: \", sess.run(global_step), 'episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
