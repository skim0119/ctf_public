{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On Policy\n",
    "\n",
    "### Sampling\n",
    "- [ ] Mini-batch to update 'average' gradient\n",
    "- [ ] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Research on '_bootstrap_' instead of end-reward\n",
    "- Add global step\n",
    "- Think about adding discont to advantage\n",
    "- Normalize reward?\n",
    "- Record method in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='A3C_lstm_try'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skim0119/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import signal\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder_v2 as one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards\n",
    "from utility.buffer import Trajectory, Trajectory_buffer\n",
    "\n",
    "from network.ActorCritic_lstm import ActorCritic as Network\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing global configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Environment\n",
    "action_space = config.getint('DEFAULT','ACTION_SPACE')\n",
    "n_agent = config.getint('DEFAULT','NUM_AGENT')\n",
    "map_size = config.getint('DEFAULT','MAP_SIZE')\n",
    "vision_range = config.getint('DEFAULT','VISION_RANGE')\n",
    "\n",
    "## Training\n",
    "total_episodes = config.getint('TRAINING','TOTAL_EPISODES')\n",
    "max_ep = config.getint('TRAINING','MAX_STEP')\n",
    "critic_beta = config.getfloat('TRAINING', 'CRITIC_BETA')\n",
    "entropy_beta = config.getfloat('TRAINING', 'ENTROPY_BETA')\n",
    "gamma = config.getfloat('TRAINING', 'DISCOUNT_RATE')\n",
    "\n",
    "decay_lr = config.getboolean('TRAINING','DECAYING_LR')\n",
    "lr_a = 1e-3 # config.getfloat('TRAINING','LR_ACTOR')\n",
    "lr_c = 1e-2 # config.getfloat('TRAINING','LR_CRITIC')\n",
    "\n",
    "## Save/Summary\n",
    "save_network_frequency = config.getint('TRAINING','SAVE_NETWORK_FREQ')\n",
    "save_stat_frequency = config.getint('TRAINING','SAVE_STATISTICS_FREQ')\n",
    "moving_average_step = config.getint('TRAINING','MOVING_AVERAGE_SIZE')\n",
    "\n",
    "## GPU\n",
    "gpu_capacity = config.getfloat('GPU_CONFIG','GPU_CAPACITY')\n",
    "gpu_allowgrow = config.getboolean('GPU_CONFIG', 'GPU_ALLOWGROW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local configuration parameters\n",
    "po_transition = 100000 # Partial observable\n",
    "#serial_length = 8\n",
    "\n",
    "# Env Settings\n",
    "n_channel = 11\n",
    "vision_dx, vision_dy = 2*vision_range+1, 2*vision_range+1\n",
    "in_size = [None,vision_dx,vision_dy,n_channel]\n",
    "nenv = 8 #(int) (multiprocessing.cpu_count())\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Network Structure\n",
    "\n",
    "![Network Structure](https://cdn-images-1.medium.com/max/1600/1*YtnGhtSAMnnHSL8PvS7t_w.png)\n",
    "\n",
    "- Network is given in network.ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "![Environment Interaction Diagram](https://cdn-images-1.medium.com/max/1600/1*Hzql_1t0-wwDxiz0C97AcQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the sessi\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(threading.Thread):\n",
    "    def __init__(self, name, global_network, sess, global_step, coord):\n",
    "        super(Environment, self).__init__()\n",
    "        # Initialize Environment worker\n",
    "        self.env = gym.make(\"cap-v0\").unwrapped\n",
    "        self.name = name\n",
    "        self.global_network = global_network\n",
    "        self.sess = sess\n",
    "        self.global_step = global_step\n",
    "        self.coord = coord\n",
    "        \n",
    "        # Create AC Network for Worker\n",
    "        self.local_network = Network(in_size=in_size,\n",
    "                                     action_size=action_space,\n",
    "                                     lr_actor=lr_a,\n",
    "                                     lr_critic=lr_c,\n",
    "                                     scope=self.name,\n",
    "                                     #grad_clip_norm=10.0,\n",
    "                                     global_step=global_step,\n",
    "                                     entropy_beta=entropy_beta,\n",
    "                                     sess=sess,\n",
    "                                     global_network=global_ac)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def run(self, saver, writer):\n",
    "        \"\"\"Override Thread.run\n",
    "\n",
    "        Note:\n",
    "            Loop to run rollout\n",
    "            Include summarizing and save\n",
    "        \"\"\"\n",
    "        self.saver = saver\n",
    "        self.writer = writer\n",
    "        \n",
    "        global global_rewards, global_ep_rewards, global_length, global_succeed, global_episodes\n",
    "        total_step = 0\n",
    "        while not self.coord.should_stop() and global_episodes < total_episodes:\n",
    "            self.rollout(init_step=total_step)\n",
    "            \n",
    "    def get_action(self, states, rnn_states):\n",
    "        \"\"\"Run graph to get action\n",
    "\n",
    "        Args:\n",
    "            state (list): list of state for each agent\n",
    "            rnn_states (list): list of rnn inputs for each agent\n",
    "\n",
    "        Returns:\n",
    "            action (list) : The action for each egent\n",
    "            values (list) : The value for each action for each agent\n",
    "            rnn_next (list) : List of next rnn state for each agent\n",
    "\n",
    "        Note:\n",
    "            If rnn_states=None, get action without rnn_states.\n",
    "        \"\"\"\n",
    "        actions, values, final_states = self.local_network.feed_forward(\n",
    "            state=np.expand_dims(states,axis=1),\n",
    "            rnn_init_state=rnn_states,\n",
    "            seq_len=[1]*len(states)\n",
    "            )\n",
    "            \n",
    "        self.feed_dict = {self.local_network.state_input_: np.expand_dims(states, axis=1),\n",
    "                          self.local_network.rnn_init_states_: self.local_network.get_lstm_initial(4),\n",
    "                          self.local_network.seq_len_: [1]*n_agent}\n",
    "        return actions, values, final_states\n",
    "            \n",
    "    def rollout(self, init_step=0):\n",
    "        global global_episodes\n",
    "        total_step = init_step\n",
    "        with self.sess.as_default(), self.sess.graph.as_default():\n",
    "            # Initialize run\n",
    "            s0 = self.env.reset(map_size=map_size,\n",
    "                                #seed=int(global_episodes % 10) + 10,\n",
    "                                policy_red=policy.zeros.PolicyGen(self.env.get_map,\n",
    "                                                                  self.env.get_team_red))\n",
    "            if po_transition < global_episodes:\n",
    "                s0 = one_hot_encoder(s0, self.env.get_team_blue, vision_range)\n",
    "            else:\n",
    "                s0 = one_hot_encoder(self.env._env, self.env.get_team_blue, vision_range)\n",
    "\n",
    "            # parameters \n",
    "            ep_r = 0 # Episodic Reward\n",
    "            prev_r = 0\n",
    "            step = 0\n",
    "            d = False\n",
    "            \n",
    "            # Trajectory Buffers\n",
    "            trajs = [Trajectory(depth=6) for _ in range(n_agent)]\n",
    "\n",
    "            # RNN Initializer\n",
    "            self.rnn_states = self.local_network.get_lstm_initial(n_agent)\n",
    "\n",
    "            # Bootstrap\n",
    "            a1, v1, final_states = self.get_action(s0, self.rnn_states)\n",
    "            is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "            \n",
    "            while step <= max_ep and not d:\n",
    "                a, v0 = a1, v1\n",
    "                was_alive = is_alive\n",
    "                self.rnn_states = np.asarray(final_states)\n",
    "\n",
    "                s1, rc, d, _ = self.env.step(a)\n",
    "                if po_transition < global_episodes:\n",
    "                    s1 = one_hot_encoder(s1, self.env.get_team_blue, vision_range)\n",
    "                else:\n",
    "                    s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, vision_range)\n",
    "                is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "                r = rc - prev_r - 0.5\n",
    "                \n",
    "                if step == max_ep and d == False:\n",
    "                    r = -100\n",
    "                    rc = -100\n",
    "                    d = True\n",
    "\n",
    "                r /= 100.0\n",
    "                ep_r += r\n",
    "\n",
    "                if d:\n",
    "                    v1 = [0.0 for _ in range(n_agent)]\n",
    "                else:\n",
    "                    a1, v1, final_states = self.get_action(s1, self.rnn_states)\n",
    "\n",
    "                # push to buffer\n",
    "                for idx, agent in enumerate(self.env.get_team_blue):\n",
    "                    if was_alive[idx]:\n",
    "                        trajs[idx].append([s0[idx],\n",
    "                                           a[idx],\n",
    "                                           r,\n",
    "                                           v0[idx],\n",
    "                                           0,\n",
    "                                           self.rnn_states[:,idx,:]\n",
    "                                          ])\n",
    "\n",
    "                # Iteration\n",
    "                prev_r = rc\n",
    "                total_step += 1\n",
    "                step += 1\n",
    "                s0 = s1\n",
    "                \n",
    "                # self.env.render(mode='fast')\n",
    "            print(self.env.blue_win, ep_r, step)\n",
    "            #self.local_network.pull_global()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_vars(sess):\n",
    "    from itertools import compress\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([~(tf.is_variable_initialized(var)) \\\n",
    "                                   for var in global_vars])\n",
    "    not_initialized_vars = list(compress(global_vars, is_not_initialized))\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skim0119/anaconda3/lib/python3.6/site-packages/gym-0.10.5-py3.6.egg/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/A3C_lstm_try/ctf_policy.ckpt-10000\n",
      "Load Model :  ./model/A3C_lstm_try/ctf_policy.ckpt-10000\n",
      "False -1.8750000000000007 151\n",
      "True 0.945 11\n",
      "False -2.000000000000001 151\n",
      "False -1.7500000000000004 151\n",
      "True 0.74 52\n",
      "True 0.965 7\n",
      "True 0.875 25\n",
      "True 0.92 16\n",
      "False -1.7500000000000004 151\n",
      "False -2.000000000000001 151\n",
      "False -1.8750000000000007 151\n",
      "False -1.7500000000000004 151\n",
      "True 0.9 20\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.6250000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -2.0000000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -2.0000000000000004 151\n",
      "False -1.7500000000000007 151\n",
      "False -1.8750000000000004 151\n",
      "True 0.925 15\n",
      "False -2.124999999999998 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000007 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000007 151\n",
      "False -1.7500000000000004 151\n",
      "False -2.000000000000001 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "True 0.945 11\n",
      "False -2.0000000000000004 151\n",
      "False -2.0000000000000004 151\n",
      "True 0.87 26\n",
      "False -2.000000000000001 151\n",
      "False -2.0000000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -2.000000000000001 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.6250000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.6250000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.1800000000000002 36\n",
      "False -1.8750000000000004 151\n",
      "True 0.8899999999999999 22\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "True 0.905 19\n",
      "False -1.7500000000000004 151\n",
      "True 0.96 8\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -2.124999999999998 151\n",
      "False -2.000000000000001 151\n",
      "True 0.845 31\n",
      "True 0.915 17\n",
      "False -1.8750000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -2.000000000000001 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "True 0.955 9\n",
      "True 0.855 29\n",
      "True 0.865 27\n",
      "False -1.8750000000000007 151\n",
      "False -2.124999999999998 151\n",
      "True 0.865 27\n",
      "False -2.0000000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -2.0000000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -2.0000000000000004 151\n",
      "False -2.000000000000001 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "True 0.8200000000000001 36\n",
      "False -1.8750000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -2.124999999999998 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "True 0.89 22\n",
      "True 0.94 12\n",
      "True 0.885 23\n",
      "False -2.124999999999998 151\n",
      "False -2.124999999999998 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -2.0000000000000004 151\n",
      "False -2.124999999999998 151\n",
      "False -2.000000000000001 151\n",
      "False -1.2400000000000002 48\n",
      "False -2.000000000000001 151\n",
      "False -2.124999999999998 151\n",
      "False -1.6250000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "True 0.8399999999999999 32\n",
      "False -2.000000000000001 151\n",
      "False -2.000000000000001 151\n",
      "False -1.8750000000000004 151\n",
      "True 0.895 21\n",
      "False -2.0000000000000004 151\n",
      "False -1.8750000000000007 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.8750000000000004 151\n",
      "False -1.7500000000000004 151\n",
      "False -1.7500000000000004 151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3aec3180d4e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-a5fa93eafc9c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, saver, writer)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_episodes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_episodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a5fa93eafc9c>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, init_step)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;31m# push to buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a5fa93eafc9c>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, states, rnn_states)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mrnn_init_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/network/ActorCritic_lstm.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, state, rnn_init_state, seq_len)\u001b[0m\n\u001b[1;32m    325\u001b[0m                      self.seq_len_: seq_len}\n\u001b[1;32m    326\u001b[0m         action_prob, critic, final_state = self.sess.run(\n\u001b[0;32m--> 327\u001b[0;31m             [self.action, self.critic, self.final_state], feed_dict)\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_prob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coord = tf.train.Coordinator()\n",
    "# Global Network\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "#global_step_next = tf.assign_add(global_step, 1)\n",
    "global_ac = Network(in_size=in_size,\n",
    "                    action_size=action_space,\n",
    "                    scope=global_scope,\n",
    "                    sess=sess,\n",
    "                    global_step=global_step)\n",
    "\n",
    "# Local workers\n",
    "workers = []\n",
    "# loop for each workers\n",
    "\n",
    "for idx in range(nenv):#tqdm(range(nenv), ncols=65, desc=\"Process Initiate\"):\n",
    "    name = 'W_%i' % idx\n",
    "    workers.append(Environment(name, global_ac, sess, global_step=global_step, coord=coord))\n",
    "\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    #tf.reset_default_graph()\n",
    "    #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    #saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path+'.meta', clear_devices=True)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    #initialize_uninitialized_vars(sess)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "worker_threads = []\n",
    "global_episodes = 0#sess.run(global_step)\n",
    "\n",
    "worker = workers[0]\n",
    "worker.run(saver, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
