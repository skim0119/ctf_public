{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On Policy\n",
    "\n",
    "### Sampling\n",
    "- [ ] Mini-batch to update 'average' gradient\n",
    "- [ ] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Research on '_bootstrap_' instead of end-reward\n",
    "- Add global step\n",
    "- Think about adding discont to advantage\n",
    "- Normalize reward?\n",
    "- Record method in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/A3C_lstm/ model/A3C_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='A3C_lstm'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "import signal\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder_v2 as one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards\n",
    "from utility.buffer import Trajectory, Trajectory_buffer\n",
    "\n",
    "from network.ActorCritic_lstm import ActorCritic as Network\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing global configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Environment\n",
    "action_space = config.getint('DEFAULT','ACTION_SPACE')\n",
    "n_agent = config.getint('DEFAULT','NUM_AGENT')\n",
    "map_size = config.getint('DEFAULT','MAP_SIZE')\n",
    "vision_range = config.getint('DEFAULT','VISION_RANGE')\n",
    "\n",
    "## Training\n",
    "total_episodes = config.getint('TRAINING','TOTAL_EPISODES')\n",
    "max_ep = config.getint('TRAINING','MAX_STEP')\n",
    "critic_beta = config.getfloat('TRAINING', 'CRITIC_BETA')\n",
    "entropy_beta = config.getfloat('TRAINING', 'ENTROPY_BETA')\n",
    "gamma = config.getfloat('TRAINING', 'DISCOUNT_RATE')\n",
    "\n",
    "decay_lr = config.getboolean('TRAINING','DECAYING_LR')\n",
    "lr_a = 5e-5 # config.getfloat('TRAINING','LR_ACTOR')\n",
    "lr_c = 4e-4 # config.getfloat('TRAINING','LR_CRITIC')\n",
    "\n",
    "## Save/Summary\n",
    "save_network_frequency = config.getint('TRAINING','SAVE_NETWORK_FREQ')\n",
    "save_stat_frequency = config.getint('TRAINING','SAVE_STATISTICS_FREQ')\n",
    "moving_average_step = config.getint('TRAINING','MOVING_AVERAGE_SIZE')\n",
    "\n",
    "## GPU\n",
    "gpu_capacity = config.getfloat('GPU_CONFIG','GPU_CAPACITY')\n",
    "gpu_allowgrow = config.getboolean('GPU_CONFIG', 'GPU_ALLOWGROW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local configuration parameters\n",
    "po_transition = 50000 # Partial observable\n",
    "serial_length = 16\n",
    "\n",
    "# Env Settings\n",
    "n_channel = 11\n",
    "vision_dx, vision_dy = 2*vision_range+1, 2*vision_range+1\n",
    "in_size = [None,vision_dx,vision_dy,n_channel]\n",
    "nenv = 12#(int) (multiprocessing.cpu_count())\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Network Structure\n",
    "\n",
    "![Network Structure](https://cdn-images-1.medium.com/max/1600/1*YtnGhtSAMnnHSL8PvS7t_w.png)\n",
    "\n",
    "- Network is given in network.ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "![Environment Interaction Diagram](https://cdn-images-1.medium.com/max/1600/1*Hzql_1t0-wwDxiz0C97AcQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_capacity,\n",
    "                            allow_growth=gpu_allowgrow)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(threading.Thread):\n",
    "    def __init__(self, name, global_network, sess, global_step, coord):\n",
    "        super(Environment, self).__init__()\n",
    "        # Initialize Environment worker\n",
    "        self.env = gym.make(\"cap-v0\").unwrapped\n",
    "        self.name = name\n",
    "        self.global_network = global_network\n",
    "        self.sess = sess\n",
    "        self.global_step = global_step\n",
    "        self.coord = coord\n",
    "        \n",
    "        # Create AC Network for Worker\n",
    "        self.local_network = Network(in_size=in_size,\n",
    "                                     action_size=action_space,\n",
    "                                     lr_actor=lr_a,\n",
    "                                     lr_critic=lr_c,\n",
    "                                     scope=self.name,\n",
    "                                     global_step=global_step,\n",
    "                                     entropy_beta=entropy_beta,\n",
    "                                     sess=sess,\n",
    "                                     global_network=global_ac)\n",
    "        \n",
    "\n",
    "        \n",
    "    def run(self, saver, writer):\n",
    "        \"\"\"Override Thread.run\n",
    "\n",
    "        Note:\n",
    "            Loop to run rollout\n",
    "            Include summarizing and save\n",
    "        \"\"\"\n",
    "        self.saver = saver\n",
    "        self.writer = writer\n",
    "        \n",
    "        global global_rewards, global_ep_rewards, global_length, global_succeed, global_episodes\n",
    "        total_step = 0\n",
    "        while not coord.should_stop() and global_episodes < total_episodes:\n",
    "            ep_r, r, l, s, aloss, closs, etrpy, summary_ = self.rollout(init_step=total_step)\n",
    "            total_step += l\n",
    "            \n",
    "            global_ep_rewards.append(ep_r)\n",
    "            global_rewards.append(r)\n",
    "            global_length.append(l)\n",
    "            global_succeed.append(s)\n",
    "            \n",
    "            global_episodes += 1\n",
    "            self.sess.run(global_step_next)\n",
    "            progbar.update(global_episodes)\n",
    "            \n",
    "            if global_episodes % save_stat_frequency == 0 and global_episodes != 0:\n",
    "                summary = tf.Summary()\n",
    "                summary.value.add(tag='Records/mean_reward', simple_value=global_rewards())\n",
    "                summary.value.add(tag='Records/mean_length', simple_value=global_length())\n",
    "                summary.value.add(tag='Records/mean_succeed', simple_value=global_succeed())\n",
    "                summary.value.add(tag='Records/mean_episode_reward', simple_value=global_ep_rewards())\n",
    "                summary.value.add(tag='summary/Entropy', simple_value=etrpy)\n",
    "                summary.value.add(tag='summary/actor_loss', simple_value=aloss)\n",
    "                summary.value.add(tag='summary/critic_loss', simple_value=closs)\n",
    "                self.writer.add_summary(summary,global_episodes)\n",
    "                self.writer.add_summary(summary_,global_episodes)\n",
    "\n",
    "                self.writer.flush()\n",
    "                \n",
    "            if global_episodes % save_network_frequency == 0 and global_episodes != 0:\n",
    "                self.saver.save(self.sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "                \n",
    "    \n",
    "    def get_action(self, state, rnn_states):\n",
    "        \"\"\"Run graph to get action\n",
    "\n",
    "        Args:\n",
    "            state (list): list of state for each agent\n",
    "            rnn_states (list): list of rnn inputs for each agent\n",
    "\n",
    "        Returns:\n",
    "            action (list) : The action for each egent\n",
    "            values (list) : The value for each action for each agent\n",
    "            rnn_next (list) : List of next rnn state for each agent\n",
    "\n",
    "        Note:\n",
    "            If rnn_states=None, get action without rnn_states.\n",
    "        \"\"\"\n",
    "        actions, values = [], []\n",
    "        final_states = []\n",
    "        for state, rnn_state in zip(states, rnn_states):\n",
    "            action, value, final_state = self.local_network.feed_forward(\n",
    "                states=state[np.newaxis,np.newaxis,:],\n",
    "                rnn_init_states=rnn_state\n",
    "                )\n",
    "            actions.append(action)\n",
    "            values.append(value)\n",
    "            final_states.append(final_state)\n",
    "        print(actions, values)\n",
    "        input('press <enter>')  # debug\n",
    "\n",
    "        return actions, values, final_states\n",
    "            \n",
    "    def rollout(self, init_step=0):\n",
    "        total_step = init_step\n",
    "        with self.sess.as_default(), self.sess.graph.as_default():\n",
    "            # Initialize run\n",
    "            s1 = self.env.reset(map_size=map_size, \n",
    "                                policy_red=policy.zeros.PolicyGen(self.env.get_map,\n",
    "                                                                  self.env.get_team_red))\n",
    "            if po_transition < global_episodes:\n",
    "                s1 = one_hot_encoder(s1, self.env.get_team_blue, vision_range)\n",
    "            else:\n",
    "                s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, vision_range)\n",
    "\n",
    "            # parameters \n",
    "            ep_r = 0 # Episodic Reward\n",
    "            prev_r = 0\n",
    "            step = 0\n",
    "            d = False\n",
    "            \n",
    "            # Trajectory Buffers\n",
    "            trajs = [Trajectory(depth=5) for _ in range(n_agent)]\n",
    "            \n",
    "            #indv_history = [[] for _ in range(len(self.env.get_team_blue))]\n",
    "\n",
    "            # RNN Initialize (If lstm is off, it will remain as list of None)\n",
    "            rnn_states_serial = [tuple([] for _ in range(network.gru_num_layers)) for _ in range(n_agent)]\n",
    "            rnn_states = [self.local_network.get_lstm_initial() for _ in range(n_agent)]\n",
    "\n",
    "            # Bootstrap\n",
    "            a1, v1, rnn_states = self.get_action(s1, rnn_states)\n",
    "            is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "            \n",
    "            while step <= max_ep and not d:\n",
    "                a, v0 = a1, v1\n",
    "                s0 = s1\n",
    "                was_alive = is_alive\n",
    "\n",
    "                s1, rc, d, _ = self.env.step(a)\n",
    "                is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "                if po_transition < global_episodes:\n",
    "                    s1 = one_hot_encoder(s1, self.env.get_team_blue, vision_range)\n",
    "                else:\n",
    "                    s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, vision_range)\n",
    "\n",
    "                r = (rc - prev_r-1)\n",
    "                if step == max_ep and d == False:\n",
    "                    r = -100\n",
    "                    rc = -100\n",
    "                    d = True\n",
    "\n",
    "                r /= 100.0\n",
    "                ep_r += r\n",
    "\n",
    "                if d:\n",
    "                    v1 = [0.0 for _ in range(len(self.env.get_team_blue))]\n",
    "                else:\n",
    "                    a1, v1, rnn_states = self.get_action(s1, rnn_states)\n",
    "\n",
    "                # push to buffer\n",
    "                for idx, agent in enumerate(self.env.get_team_blue):\n",
    "                    if was_alive[idx]:\n",
    "                        trajs[idx].append([s0[idx],\n",
    "                                           a[idx],\n",
    "                                           r,\n",
    "                                           v0[idx],\n",
    "                                           0\n",
    "                                          ])\n",
    "                        for i in range(self.local_network.rnn_num_layers):\n",
    "                            rnn_states_serial[idx][i].append(rnn_states[idx][i][0])\n",
    "\n",
    "                # Iteration\n",
    "                prev_r = rc\n",
    "                total_step += 1\n",
    "                step += 1\n",
    "                \n",
    "            replay_buffer = Trajectory_buffer(depth=5)\n",
    "            trim_init_states = tuple([] for _ in range(network.gru_num_layers))\n",
    "            seq_lens = []\n",
    "            \n",
    "            for idx, traj in enumerate(trajs):\n",
    "                if len(traj)<=serial_length:\n",
    "                    continue\n",
    "                else:\n",
    "                    traj_length = len(traj)\n",
    "                    batch_length = traj_length // serial_length\n",
    "\n",
    "                # Discount Reward\n",
    "                _rew = np.array(traj[2])\n",
    "                _val = np.array(traj[3]+v1[idx])  # Bootstrap\n",
    "                _td = _rew + gamma * _val[1:]\n",
    "                _adv = _td - _val[:-1]\n",
    "                traj[3] = _td.tolist()\n",
    "                traj[4] = discount_rewards(_adv, gamma).tolist()\n",
    "                                                   \n",
    "                # Trim rnn states\n",
    "                ss = 0 if traj_length % serial_length == 0 else 1\n",
    "                rnn_init_state = rnn_states_serial[idx]\n",
    "                trim_init_state = tuple(rnn_init_state[i][::serial_length][ss:] for i in range(self.local_network.rnn_num_layers))\n",
    "\n",
    "                # Sequence length\n",
    "                seq_len = [serial_length]*(batch_length)\n",
    "\n",
    "                # Trim batch for each trajectory\n",
    "                traj_list = traj.trim(serial_length)\n",
    "                replay_buffer.extend(traj_list)\n",
    "                [a.extend(b) for a,b in zip(trim_init_states, trim_init_state)]\n",
    "                seq_lens.extend(seq_len)\n",
    "                \n",
    "            states, actions, rewards, td_targets, advantages = replay_buffer.sample()\n",
    "            init_state = np.array(trim_init_states[0])\n",
    "            seq_len = np.array(seq_lens)\n",
    "            \n",
    "            aloss, closs, etrpy = self.local_network.feed_forward(states, actions, td_targets, advantages, init_state, seq_len)                \n",
    "            # summary_ = self.sess.run(merged_summary_op, feed_dict)\n",
    "            \n",
    "            self.local_network.pull_global()\n",
    "            \n",
    "        return ep_r, rc, step, self.env.blue_win, aloss, closs, etrpy#, summary_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get likelihood of global with states\n",
    "soft_prob = self.sess.run(self.globalNetwork.actor,\n",
    "                          feed_dict={self.globalNetwork.state_input : np.stack(observations)})\n",
    "target_policy = np.array([p[action] for p, action in zip(soft_prob,actions)])\n",
    "\n",
    "retraceLambda = 0.202\n",
    "sampling_weight = []\n",
    "sampling_weight_cumulative = []\n",
    "running_prob = 1.0\n",
    "for idx, (pi, beta) in enumerate(zip(target_policy, behavior_policy)):\n",
    "    ratio = retraceLambda * min(1.0, pi / beta)\n",
    "    running_prob *= ratio\n",
    "    sampling_weight.append(ratio)\n",
    "    sampling_weight_cumulative.append(running_prob)\n",
    "sampling_weight = np.array(sampling_weight)\n",
    "sampling_weight_cumulative = np.array(sampling_weight_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes must be equal rank, but are 1 and 0\n\tFrom merging shape 0 with other shapes. for 'global/actor/stack_1' (op: 'Pack') with input shapes: [2], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1625\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 1 and 0\n\tFrom merging shape 0 with other shapes. for 'global/actor/stack_1' (op: 'Pack') with input shapes: [2], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a2bbe4bf9780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                         global_step=global_step)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Local workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/network/ActorCritic_lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_size, action_size, scope, lr_actor, lr_critic, grad_clip_norm, global_step, critic_beta, entropy_beta, sess, global_network, separate_train)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'global'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/network/ActorCritic_lstm.py\u001b[0m in \u001b[0;36m_build_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mserial_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             bulk_shape = tf.stack([tf.shape(self.state_input_)[0:2],\n\u001b[0;32m--> 142\u001b[0;31m                                    self.serial_size])\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mserial_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mserial_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserial_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbulk_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                                       expanded_num_dims))\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   4687\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4688\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4689\u001b[0;31m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   4690\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4691\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3270\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3272\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3273\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1788\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1789\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1790\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ctf_public/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes must be equal rank, but are 1 and 0\n\tFrom merging shape 0 with other shapes. for 'global/actor/stack_1' (op: 'Pack') with input shapes: [2], []."
     ]
    }
   ],
   "source": [
    "coord = tf.train.Coordinator()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Global Network\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    global_step_next = tf.assign_add(global_step, 1)\n",
    "    global_ac = Network(in_size=in_size,\n",
    "                        action_size=action_space,\n",
    "                        scope=global_scope,\n",
    "                        sess=sess,\n",
    "                        global_step=global_step)\n",
    "\n",
    "    # Local workers\n",
    "    workers = []\n",
    "    # loop for each workers\n",
    "\n",
    "    for idx in range(nenv):\n",
    "        name = 'W_%i' % idx\n",
    "        workers.append(Environment(name, global_ac, sess, global_step=global_step, coord=coord))\n",
    "        print(f'worker: {name} initiated')\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")\n",
    "    \n",
    "\n",
    "worker_threads = []\n",
    "global_episodes = sess.run(global_step)\n",
    "\n",
    "# Summarize\n",
    "for var in tf.trainable_variables(scope=global_scope):\n",
    "    tf.summary.histogram(var.name, var)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "for worker in workers:\n",
    "    job = lambda: worker.run(saver, writer)\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
