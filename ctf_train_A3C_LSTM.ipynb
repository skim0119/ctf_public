{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On Policy\n",
    "\n",
    "### Sampling\n",
    "- [ ] Mini-batch to update 'average' gradient\n",
    "- [ ] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Research on '_bootstrap_' instead of end-reward\n",
    "- Add global step\n",
    "- Think about adding discont to advantage\n",
    "- Normalize reward?\n",
    "- Record method in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/A3C_lstm/ model/A3C_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='A3C_lstm'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import signal\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder_v2 as one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards\n",
    "from utility.buffer import Trajectory, Trajectory_buffer\n",
    "\n",
    "from network.ActorCritic_lstm import ActorCritic as Network\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing global configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Environment\n",
    "action_space = config.getint('DEFAULT','ACTION_SPACE')\n",
    "n_agent = config.getint('DEFAULT','NUM_AGENT')\n",
    "map_size = config.getint('DEFAULT','MAP_SIZE')\n",
    "vision_range = config.getint('DEFAULT','VISION_RANGE')\n",
    "\n",
    "## Training\n",
    "total_episodes = config.getint('TRAINING','TOTAL_EPISODES')\n",
    "max_ep = config.getint('TRAINING','MAX_STEP')\n",
    "critic_beta = config.getfloat('TRAINING', 'CRITIC_BETA')\n",
    "entropy_beta = config.getfloat('TRAINING', 'ENTROPY_BETA')\n",
    "gamma = config.getfloat('TRAINING', 'DISCOUNT_RATE')\n",
    "\n",
    "decay_lr = config.getboolean('TRAINING','DECAYING_LR')\n",
    "lr_a = 1e-3 # config.getfloat('TRAINING','LR_ACTOR')\n",
    "lr_c = 2e-3 # config.getfloat('TRAINING','LR_CRITIC')\n",
    "\n",
    "## Save/Summary\n",
    "save_network_frequency = config.getint('TRAINING','SAVE_NETWORK_FREQ')\n",
    "save_stat_frequency = config.getint('TRAINING','SAVE_STATISTICS_FREQ')\n",
    "moving_average_step = config.getint('TRAINING','MOVING_AVERAGE_SIZE')\n",
    "\n",
    "## GPU\n",
    "gpu_capacity = config.getfloat('GPU_CONFIG','GPU_CAPACITY')\n",
    "gpu_allowgrow = config.getboolean('GPU_CONFIG', 'GPU_ALLOWGROW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local configuration parameters\n",
    "po_transition = 100000 # Partial observable\n",
    "serial_length = 8\n",
    "\n",
    "# Env Settings\n",
    "n_channel = 11\n",
    "vision_dx, vision_dy = 2*vision_range+1, 2*vision_range+1\n",
    "in_size = [None,vision_dx,vision_dy,n_channel]\n",
    "nenv = 12#(int) (multiprocessing.cpu_count())\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Network Structure\n",
    "\n",
    "![Network Structure](https://cdn-images-1.medium.com/max/1600/1*YtnGhtSAMnnHSL8PvS7t_w.png)\n",
    "\n",
    "- Network is given in network.ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "![Environment Interaction Diagram](https://cdn-images-1.medium.com/max/1600/1*Hzql_1t0-wwDxiz0C97AcQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_capacity,\n",
    "                            allow_growth=gpu_allowgrow)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(threading.Thread):\n",
    "    def __init__(self, name, global_network, sess, global_step, coord):\n",
    "        super(Environment, self).__init__()\n",
    "        # Initialize Environment worker\n",
    "        self.env = gym.make(\"cap-v0\").unwrapped\n",
    "        self.name = name\n",
    "        self.global_network = global_network\n",
    "        self.sess = sess\n",
    "        self.global_step = global_step\n",
    "        self.coord = coord\n",
    "        \n",
    "        # Create AC Network for Worker\n",
    "        self.local_network = Network(in_size=in_size,\n",
    "                                     action_size=action_space,\n",
    "                                     lr_actor=lr_a,\n",
    "                                     lr_critic=lr_c,\n",
    "                                     scope=self.name,\n",
    "                                     grad_clip_norm=50.0,\n",
    "                                     global_step=global_step,\n",
    "                                     entropy_beta=entropy_beta,\n",
    "                                     sess=sess,\n",
    "                                     global_network=global_ac)\n",
    "        \n",
    "\n",
    "        \n",
    "    def run(self, saver, writer):\n",
    "        \"\"\"Override Thread.run\n",
    "\n",
    "        Note:\n",
    "            Loop to run rollout\n",
    "            Include summarizing and save\n",
    "        \"\"\"\n",
    "        self.saver = saver\n",
    "        self.writer = writer\n",
    "        \n",
    "        global global_rewards, global_ep_rewards, global_length, global_succeed, global_episodes\n",
    "        total_step = 0\n",
    "        while not coord.should_stop() and global_episodes < total_episodes:\n",
    "            ep_r, r, l, s, aloss, closs, etrpy = self.rollout(init_step=total_step)\n",
    "            if etrpy == None:\n",
    "                continue\n",
    "            total_step += l\n",
    "            \n",
    "            global_ep_rewards.append(ep_r)\n",
    "            global_rewards.append(r)\n",
    "            global_length.append(l)\n",
    "            global_succeed.append(s)\n",
    "            \n",
    "            global_episodes += 1\n",
    "            self.sess.run(global_step_next)\n",
    "            progbar.update(global_episodes)\n",
    "            \n",
    "            if global_episodes % save_stat_frequency == 0 and global_episodes != 0:\n",
    "                summary = tf.Summary()\n",
    "                summary.value.add(tag='Records/mean_reward', simple_value=global_rewards())\n",
    "                summary.value.add(tag='Records/mean_length', simple_value=global_length())\n",
    "                summary.value.add(tag='Records/mean_succeed', simple_value=global_succeed())\n",
    "                summary.value.add(tag='Records/mean_episode_reward', simple_value=global_ep_rewards())\n",
    "                summary.value.add(tag='summary/Entropy', simple_value=etrpy)\n",
    "                summary.value.add(tag='summary/actor_loss', simple_value=aloss)\n",
    "                summary.value.add(tag='summary/critic_loss', simple_value=closs)\n",
    "                self.writer.add_summary(summary,global_episodes)\n",
    "                #self.writer.add_summary(summary_,global_episodes)\n",
    "\n",
    "                self.writer.flush()\n",
    "                \n",
    "            if global_episodes % save_network_frequency == 0 and global_episodes != 0:\n",
    "                self.saver.save(self.sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "                \n",
    "    \n",
    "    def get_action(self, states, rnn_states):\n",
    "        \"\"\"Run graph to get action\n",
    "\n",
    "        Args:\n",
    "            state (list): list of state for each agent\n",
    "            rnn_states (list): list of rnn inputs for each agent\n",
    "\n",
    "        Returns:\n",
    "            action (list) : The action for each egent\n",
    "            values (list) : The value for each action for each agent\n",
    "            rnn_next (list) : List of next rnn state for each agent\n",
    "\n",
    "        Note:\n",
    "            If rnn_states=None, get action without rnn_states.\n",
    "        \"\"\"\n",
    "        actions, values = [], []\n",
    "        final_states = []\n",
    "        for state, rnn_state in zip(states, rnn_states):\n",
    "            action, value, final_state = self.local_network.feed_forward(\n",
    "                state=state[np.newaxis,np.newaxis,:],\n",
    "                rnn_init_state=rnn_state\n",
    "                )\n",
    "            actions.append(action[0])\n",
    "            values.append(value[0])\n",
    "            final_states.append(final_state)\n",
    "        return actions, values, final_states\n",
    "            \n",
    "    def rollout(self, init_step=0):\n",
    "        total_step = init_step\n",
    "        with self.sess.as_default(), self.sess.graph.as_default():\n",
    "            # Initialize run\n",
    "            s1 = self.env.reset(map_size=map_size, \n",
    "                                policy_red=policy.zeros.PolicyGen(self.env.get_map,\n",
    "                                                                  self.env.get_team_red))\n",
    "            if po_transition < global_episodes:\n",
    "                s1 = one_hot_encoder(s1, self.env.get_team_blue, vision_range)\n",
    "            else:\n",
    "                s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, vision_range)\n",
    "\n",
    "            # parameters \n",
    "            ep_r = 0 # Episodic Reward\n",
    "            prev_r = 0\n",
    "            step = 0\n",
    "            d = False\n",
    "            \n",
    "            # Trajectory Buffers\n",
    "            trajs = [Trajectory(depth=5) for _ in range(n_agent)]\n",
    "\n",
    "            # RNN Initialize (If lstm is off, it will remain as list of None)\n",
    "            rnn_states_serial = [tuple([] for _ in range(self.local_network.rnn_num_layers)) for _ in range(n_agent)]\n",
    "            rnn_states = [self.local_network.get_lstm_initial() for _ in range(n_agent)]\n",
    "\n",
    "            # Bootstrap\n",
    "            a1, v1, final_states = self.get_action(s1, rnn_states)\n",
    "            is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "            \n",
    "            while step <= max_ep and not d:\n",
    "                a, v0 = a1, v1\n",
    "                s0 = s1\n",
    "                was_alive = is_alive\n",
    "                rnn_states = final_states\n",
    "\n",
    "                s1, rc, d, _ = self.env.step(a)\n",
    "                is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "                if po_transition < global_episodes:\n",
    "                    s1 = one_hot_encoder(s1, self.env.get_team_blue, vision_range)\n",
    "                else:\n",
    "                    s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, vision_range)\n",
    "\n",
    "                r = rc - prev_r\n",
    "                if step == max_ep and d == False:\n",
    "                    r = -100\n",
    "                    rc = -100\n",
    "                    d = True\n",
    "\n",
    "                #r /= 100.0\n",
    "                ep_r += r\n",
    "\n",
    "                if d:\n",
    "                    v1 = [0.0 for _ in range(len(self.env.get_team_blue))]\n",
    "                else:\n",
    "                    a1, v1, final_states = self.get_action(s1, rnn_states)\n",
    "\n",
    "                # push to buffer\n",
    "                for idx, agent in enumerate(self.env.get_team_blue):\n",
    "                    if was_alive[idx]:\n",
    "                        trajs[idx].append([s0[idx],\n",
    "                                           a[idx],\n",
    "                                           r,\n",
    "                                           v0[idx],\n",
    "                                           0\n",
    "                                          ])\n",
    "                        for i in range(self.local_network.rnn_num_layers):\n",
    "                            rnn_states_serial[idx][i].append(rnn_states[idx][i][0])\n",
    "\n",
    "                # Iteration\n",
    "                prev_r = rc\n",
    "                total_step += 1\n",
    "                step += 1\n",
    "                \n",
    "            replay_buffer = Trajectory_buffer(depth=5)\n",
    "            trim_init_states = tuple([] for _ in range(self.local_network.rnn_num_layers))\n",
    "            seq_lens = []\n",
    "            \n",
    "            for idx, traj in enumerate(trajs):\n",
    "                if len(traj)<=serial_length:\n",
    "                    continue\n",
    "                else:\n",
    "                    traj_length = len(traj)\n",
    "                    batch_length = traj_length // serial_length\n",
    "\n",
    "                # Discount Reward\n",
    "                _rew = np.array(traj[2])\n",
    "                _val = np.array(traj[3]+[v1[idx]])  # Bootstrap\n",
    "                _td = _rew + gamma * _val[1:]\n",
    "                _adv = _td - _val[:-1]\n",
    "                traj[3] = _td.tolist()\n",
    "                traj[4] = discount_rewards(_adv, gamma).tolist()\n",
    "                                                   \n",
    "                # Trim rnn states\n",
    "                ss = 0 if traj_length % serial_length == 0 else 1\n",
    "                rnn_init_state = rnn_states_serial[idx]\n",
    "                trim_init_state = tuple(rnn_init_state[i][::serial_length][ss:] for i in range(self.local_network.rnn_num_layers))\n",
    "\n",
    "                # Sequence length\n",
    "                seq_len = [serial_length]*(batch_length)\n",
    "\n",
    "                # Trim batch for each trajectory\n",
    "                traj_list = traj.trim(serial_length)\n",
    "                replay_buffer.extend(traj_list)\n",
    "                [a.extend(b) for a,b in zip(trim_init_states, trim_init_state)]\n",
    "                seq_lens.extend(seq_len)\n",
    "            \n",
    "            if len(replay_buffer) == 0:\n",
    "                return 0,0,0,0,0,0,None\n",
    "            states, actions, rewards, td_targets, advantages = replay_buffer.sample()\n",
    "            init_state = np.array(trim_init_states)\n",
    "            seq_len = np.array(seq_lens)\n",
    "            \n",
    "            aloss, closs, etrpy = self.local_network.feed_backward(states, actions, td_targets, advantages, init_state, seq_len)                \n",
    "            #summary_ = self.sess.run(merged_summary_op, feed_dict)\n",
    "            \n",
    "            self.local_network.pull_global()\n",
    "            \n",
    "        return ep_r, rc, step, self.env.blue_win, aloss, closs, etrpy #, summary_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get likelihood of global with states\n",
    "soft_prob = self.sess.run(self.globalNetwork.actor,\n",
    "                          feed_dict={self.globalNetwork.state_input : np.stack(observations)})\n",
    "target_policy = np.array([p[action] for p, action in zip(soft_prob,actions)])\n",
    "\n",
    "retraceLambda = 0.202\n",
    "sampling_weight = []\n",
    "sampling_weight_cumulative = []\n",
    "running_prob = 1.0\n",
    "for idx, (pi, beta) in enumerate(zip(target_policy, behavior_policy)):\n",
    "    ratio = retraceLambda * min(1.0, pi / beta)\n",
    "    running_prob *= ratio\n",
    "    sampling_weight.append(ratio)\n",
    "    sampling_weight_cumulative.append(running_prob)\n",
    "sampling_weight = np.array(sampling_weight)\n",
    "sampling_weight_cumulative = np.array(sampling_weight_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Initiate:  17%|█▊         | 2/12 [00:03<00:15,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Route Gradient\n",
      "Tensor(\"W_1/local_grad/clip_by_value:0\", shape=(5, 5, 11, 32), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/Conv/weights:0' shape=(5, 5, 11, 32) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_1:0\", shape=(32,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/Conv/biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_2:0\", shape=(3, 3, 32, 64), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/Conv_1/weights:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_3:0\", shape=(64,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/Conv_1/biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_4:0\", shape=(2, 2, 64, 64), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/Conv_2/weights:0' shape=(2, 2, 64, 64) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_5:0\", shape=(64,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/Conv_2/biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_6:0\", shape=(1024, 256), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/fully_connected/weights:0' shape=(1024, 256) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_7:0\", shape=(256,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/fully_connected/biases:0' shape=(256,) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_8:0\", shape=(512, 512), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0' shape=(512, 512) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_9:0\", shape=(512,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0' shape=(512,) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_10:0\", shape=(512, 256), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0' shape=(512, 256) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_11:0\", shape=(256,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0' shape=(256,) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_12:0\", shape=(256, 5), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/logit/weights:0' shape=(256, 5) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_13:0\", shape=(5,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/actor/logit/biases:0' shape=(5,) dtype=float32_ref>\n",
      "Critic Route Gradient\n",
      "Tensor(\"W_1/local_grad/clip_by_value_14:0\", shape=(256, 1), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/critic/fully_connected/weights:0' shape=(256, 1) dtype=float32_ref>\n",
      "Tensor(\"W_1/local_grad/clip_by_value_15:0\", shape=(1,), dtype=float32, device=/device:CPU:0) <tf.Variable 'W_1/critic/fully_connected/biases:0' shape=(1,) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Initiate: 100%|██████████| 12/12 [00:17<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Variables\n",
      "INFO:tensorflow:Summary name global/actor/Conv/weights:0 is illegal; using global/actor/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/Conv/biases:0 is illegal; using global/actor/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/Conv_1/weights:0 is illegal; using global/actor/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/Conv_1/biases:0 is illegal; using global/actor/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/Conv_2/weights:0 is illegal; using global/actor/Conv_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/Conv_2/biases:0 is illegal; using global/actor/Conv_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/fully_connected/weights:0 is illegal; using global/actor/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/fully_connected/biases:0 is illegal; using global/actor/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0 is illegal; using global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0 is illegal; using global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0 is illegal; using global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0 is illegal; using global/actor/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/logit/weights:0 is illegal; using global/actor/logit/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor/logit/biases:0 is illegal; using global/actor/logit/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/fully_connected/weights:0 is illegal; using global/critic/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/fully_connected/biases:0 is illegal; using global/critic/fully_connected/biases_0 instead.\n",
      "   871/150000 [..............................] - ETA: 58:57:39"
     ]
    }
   ],
   "source": [
    "coord = tf.train.Coordinator()\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Global Network\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    global_step_next = tf.assign_add(global_step, 1)\n",
    "    global_ac = Network(in_size=in_size,\n",
    "                        action_size=action_space,\n",
    "                        scope=global_scope,\n",
    "                        sess=sess,\n",
    "                        global_step=global_step)\n",
    "\n",
    "    # Local workers\n",
    "    workers = []\n",
    "    # loop for each workers\n",
    "\n",
    "    for idx in tqdm(range(nenv), ncols=65, desc=\"Process Initiate\"):\n",
    "        name = 'W_%i' % idx\n",
    "        workers.append(Environment(name, global_ac, sess, global_step=global_step, coord=coord))\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")\n",
    "    \n",
    "\n",
    "worker_threads = []\n",
    "global_episodes = sess.run(global_step)\n",
    "\n",
    "# Summarize\n",
    "for var in tf.trainable_variables(scope=global_scope):\n",
    "    tf.summary.histogram(var.name, var)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "for worker in workers:\n",
    "    job = lambda: worker.run(saver, writer)\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
