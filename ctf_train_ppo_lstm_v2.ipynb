{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On Policy\n",
    "\n",
    "### Sampling\n",
    "- [x] Mini-batch to update 'average' gradient\n",
    "- [x] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [x] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [ ] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Research on '_bootstrap_' instead of end-reward\n",
    "- Add global step\n",
    "- Think about adding discont to advantage\n",
    "- Normalize reward?\n",
    "- Record method in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/PPO_LSTM_v2/ model/PPO_LSTM_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='PPO_LSTM_v2'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.signal\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder as one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards, normalize\n",
    "from utility.buffer import Trajectory, Trajectory_buffer\n",
    "\n",
    "from network.ppo_lstm import PPO as Network\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing global configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Environment\n",
    "action_space = config.getint('DEFAULT','ACTION_SPACE')\n",
    "n_agent = 1# config.getint('DEFAULT','NUM_AGENT')\n",
    "map_size = config.getint('DEFAULT','MAP_SIZE')\n",
    "vision_range = config.getint('DEFAULT','VISION_RANGE')\n",
    "\n",
    "## Training\n",
    "total_episodes = config.getint('TRAINING','TOTAL_EPISODES')\n",
    "max_ep = config.getint('TRAINING','MAX_STEP')\n",
    "critic_beta = config.getfloat('TRAINING', 'CRITIC_BETA')\n",
    "entropy_beta = config.getfloat('TRAINING', 'ENTROPY_BETA')\n",
    "gamma = config.getfloat('TRAINING', 'DISCOUNT_RATE')\n",
    "\n",
    "decay_lr = config.getboolean('TRAINING','DECAYING_LR')\n",
    "lr_a = 1e-3 # config.getfloat('TRAINING','LR_ACTOR')\n",
    "lr_c = 2e-3 # config.getfloat('TRAINING','LR_CRITIC')\n",
    "\n",
    "## Save/Summary\n",
    "save_network_frequency = config.getint('TRAINING','SAVE_NETWORK_FREQ')\n",
    "save_stat_frequency = config.getint('TRAINING','SAVE_STATISTICS_FREQ')\n",
    "moving_average_step = config.getint('TRAINING','MOVING_AVERAGE_SIZE')\n",
    "\n",
    "## GPU\n",
    "gpu_capacity = config.getfloat('GPU_CONFIG','GPU_CAPACITY')\n",
    "gpu_allowgrow = config.getboolean('GPU_CONFIG', 'GPU_ALLOWGROW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local configuration parameters\n",
    "po_transition = 100000 # Partial observable\n",
    "batch_size = 2048*4\n",
    "\n",
    "# Env Settings\n",
    "n_channel = 6\n",
    "vision_dx, vision_dy = 2*vision_range+1, 2*vision_range+1\n",
    "in_size = [None,vision_dx*vision_dy*n_channel]\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_capacity,\n",
    "                            allow_growth=gpu_allowgrow)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"cap-v0\").unwrapped\n",
    "env.reset(map_size=map_size,\n",
    "          policy_red=policy.zeros.PolicyGen(env.get_map, env.get_team_red))\n",
    "network = Network(in_size=in_size, action_size=action_space, scope='main', sess=sess)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = []\n",
    "\n",
    "def run():\n",
    "    global global_rewards, global_ep_rewards, global_length, global_succeed\n",
    "    batch_count = 0\n",
    "    for episode in range(total_episodes + 1):\n",
    "        ep_r, r, length, batch_count, s, summary_ = rollout(init_step=batch_count, episode=episode)\n",
    "\n",
    "        global_ep_rewards.append(ep_r)\n",
    "        global_rewards.append(r)\n",
    "        global_length.append(length)\n",
    "        global_succeed.append(s)\n",
    "\n",
    "        progbar.update(episode)\n",
    "\n",
    "        if summary_ != None or (episode % save_stat_frequency == 0 and episode != 0):\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag='Records/mean_reward', simple_value=global_rewards())\n",
    "            summary.value.add(tag='Records/mean_length', simple_value=global_length())\n",
    "            summary.value.add(tag='Records/mean_succeed', simple_value=global_succeed())\n",
    "            summary.value.add(tag='Records/mean_episode_reward', simple_value=global_ep_rewards())\n",
    "            writer.add_summary(summary,episode)\n",
    "            if summary_ is not None:\n",
    "                writer.add_summary(summary_,episode)\n",
    "            writer.flush()\n",
    "\n",
    "        if episode % save_network_frequency == 0 and episode != 0:\n",
    "            saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=episode)\n",
    "\n",
    "\n",
    "def get_action(states, rnn_states):\n",
    "    \"\"\"Run graph to get action for each agents\"\"\"\n",
    "    actions, values, final_states = [], [], []\n",
    "    for idx, state in enumerate(states):            \n",
    "        action, value, final_state = network.feed_forward(\n",
    "            state=state[np.newaxis,:],\n",
    "            rnn_state=rnn_states[idx]\n",
    "            )\n",
    "        actions.append(action)\n",
    "        values.append(value)\n",
    "        final_states.append(final_state)\n",
    "\n",
    "    return actions, values, final_states\n",
    "\n",
    "def rollout(init_step=0, episode=0):\n",
    "    global experience\n",
    "    # Initialize run\n",
    "    batch_count = init_step\n",
    "    \n",
    "    s0 = env.reset()\n",
    "    if po_transition < episode:\n",
    "        s0 = one_hot_encoder(s0, env.get_team_blue, vision_range, flatten=True)\n",
    "    else:\n",
    "        s0 = one_hot_encoder(env._env, env.get_team_blue, vision_range, flatten=True)\n",
    "    # parameters\n",
    "    ep_r = 0 # Episodic Reward\n",
    "    prev_r = 0\n",
    "    step = 0\n",
    "    d = False\n",
    "\n",
    "    # Trajectory Buffers\n",
    "    trajs = [Trajectory(depth=6) for _ in range(n_agent)]\n",
    "\n",
    "    # RNN Initializer\n",
    "    rnn_states = [sess.run([network.action_eval_init_state, network.critic_eval_init_state])\n",
    "                      for _ in range(n_agent)]\n",
    "\n",
    "    # Bootstrap\n",
    "    a1, v1, final_states = get_action(s0, rnn_states)\n",
    "    is_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "    buffer_d = []\n",
    "\n",
    "    while step <= max_ep and not d:\n",
    "        a, v0 = a1, v1\n",
    "        was_alive = is_alive\n",
    "        rnn_states = final_states\n",
    "\n",
    "        s1, rc, d, _ = env.step(a)\n",
    "        if po_transition < episode:\n",
    "            s1 = one_hot_encoder(s1, env.get_team_blue, vision_range, flatten=True)\n",
    "        else:\n",
    "            s1 = one_hot_encoder(env._env, env.get_team_blue, vision_range, flatten=True)\n",
    "\n",
    "        is_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "        r = rc - prev_r - 0.01\n",
    "\n",
    "        if step == max_ep and d == False:\n",
    "            r = -100\n",
    "            rc = -100\n",
    "            d = True\n",
    "\n",
    "        r /= 100.0\n",
    "        ep_r += r\n",
    "\n",
    "        if d:\n",
    "            v1 = [0.0 for _ in range(n_agent)]\n",
    "        else:\n",
    "            a1, v1, final_states = get_action(s1, rnn_states)\n",
    "\n",
    "        # push to buffer\n",
    "        buffer_d.append(d)\n",
    "        for idx, agent in enumerate(env.get_team_blue):\n",
    "            if was_alive[idx]:\n",
    "                trajs[idx].append([s0[idx],\n",
    "                                   a[idx],\n",
    "                                   r,\n",
    "                                   v0[idx],\n",
    "                                   0,\n",
    "                                   rnn_states[idx]\n",
    "                                  ])\n",
    "\n",
    "        # Iteration\n",
    "        prev_r = rc\n",
    "        batch_count += 1\n",
    "        step += 1\n",
    "        s0 = s1    \n",
    "\n",
    "        #env.render(mode='fast')\n",
    "        \n",
    "    # Normalise rewards\n",
    "    ds = np.array(buffer_d)\n",
    "    for idx, traj in enumerate(trajs):\n",
    "        if len(traj) == 0:\n",
    "            continue\n",
    "\n",
    "        # Discount Reward\n",
    "        _rew = np.array(traj[2])\n",
    "        #_rew = np.clip(_rew / np.std(_rew), -10, 10)\n",
    "        _val = np.append(traj[3],[v1[idx]])  # Bootstrap\n",
    "        _td  = _rew + gamma * _val[1:] * (1-ds) - _val[:-1]\n",
    "        _adv = discount_rewards(_td, 0.931, mask_array=ds)\n",
    "        _ret = _adv + _val[:-1]\n",
    "        traj[3] = _td.tolist()\n",
    "        traj[4] = _adv.tolist()\n",
    "        \n",
    "        bs, ba, br, badv = traj[0], np.vstack(traj[1]), np.vstack(_ret), np.vstack(_adv) \n",
    "        #np.reshape(traj[0], [len(traj[0])] + in_size[-3:]), \n",
    "                            \n",
    "        experience.append([bs, ba, br, badv])\n",
    "    \n",
    "    # Update ppo\n",
    "    if batch_count >= batch_size:\n",
    "        # Per batch normalisation of advantages\n",
    "        advs = np.concatenate(list(zip(*experience))[3])\n",
    "        for x in experience:\n",
    "            x[3] = (x[3] - np.mean(advs)) / np.maximum(np.std(advs), 1e-6)\n",
    "\n",
    "        print(f'experience length: {len(experience)}')\n",
    "        graph_summary = network.feed_backward(experience)\n",
    "        batch_count, experience = 0, []\n",
    "    else:\n",
    "        graph_summary = None\n",
    "    \n",
    "    return ep_r, rc, step,batch_count, env.blue_win, graph_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    61/150000 [..............................] - ETA: 37:28:21experience length: 63\n",
      "Train: 2036 batches trained, 0 episodes: 94.7599470615387 sec\n",
      "   122/150000 [..............................] - ETA: 65:24:09experience length: 61\n",
      "Train: 2008 batches trained, 0 episodes: 77.29691696166992 sec\n",
      "   188/150000 [..............................] - ETA: 68:48:55experience length: 66\n",
      "Train: 2023 batches trained, 0 episodes: 98.82200956344604 sec\n",
      "   249/150000 [..............................] - ETA: 75:27:26experience length: 63\n",
      "Train: 2020 batches trained, 0 episodes: 79.2430648803711 sec\n",
      "   319/150000 [..............................] - ETA: 74:48:52experience length: 68\n",
      "Train: 2012 batches trained, 0 episodes: 99.64769172668457 sec\n",
      "   380/150000 [..............................] - ETA: 78:16:31experience length: 63\n",
      "Train: 2019 batches trained, 0 episodes: 79.63556361198425 sec\n",
      "   445/150000 [..............................] - ETA: 78:20:17experience length: 63\n",
      "Train: 2042 batches trained, 0 episodes: 96.53426480293274 sec\n",
      "   508/150000 [..............................] - ETA: 79:54:24experience length: 65\n",
      "Train: 2011 batches trained, 0 episodes: 80.44444584846497 sec\n",
      "   571/150000 [..............................] - ETA: 79:59:54experience length: 63\n",
      "Train: 2012 batches trained, 0 episodes: 99.37729287147522 sec\n",
      "   635/150000 [..............................] - ETA: 81:13:10experience length: 64\n",
      "Train: 2031 batches trained, 0 episodes: 81.38547611236572 sec\n",
      "   695/150000 [..............................] - ETA: 81:33:20experience length: 59\n",
      "Train: 2008 batches trained, 0 episodes: 96.01248550415039 sec\n",
      "   760/150000 [..............................] - ETA: 82:07:57experience length: 65\n",
      "Train: 2035 batches trained, 0 episodes: 82.8735203742981 sec\n",
      "   823/150000 [..............................] - ETA: 82:09:57experience length: 63\n",
      "Train: 2040 batches trained, 0 episodes: 100.07198095321655 sec\n",
      "   891/150000 [..............................] - ETA: 82:33:15experience length: 67\n",
      "Train: 2011 batches trained, 0 episodes: 84.39698314666748 sec\n",
      "   951/150000 [..............................] - ETA: 82:48:36experience length: 61\n",
      "Train: 2027 batches trained, 0 episodes: 99.28939580917358 sec\n",
      "  1017/150000 [..............................] - ETA: 83:12:22experience length: 65\n",
      "Train: 2031 batches trained, 0 episodes: 85.02093434333801 sec\n",
      "  1078/150000 [..............................] - ETA: 83:23:03experience length: 61\n",
      "Train: 2040 batches trained, 0 episodes: 98.85989952087402 sec\n",
      "  1143/150000 [..............................] - ETA: 83:40:45experience length: 65\n",
      "Train: 2016 batches trained, 0 episodes: 87.51711058616638 sec\n",
      "  1207/150000 [..............................] - ETA: 83:42:13experience length: 64\n",
      "Train: 2026 batches trained, 0 episodes: 103.67285180091858 sec\n",
      "  1268/150000 [..............................] - ETA: 84:19:37experience length: 61\n",
      "Train: 2022 batches trained, 0 episodes: 86.50019764900208 sec\n",
      "  1332/150000 [..............................] - ETA: 84:16:02experience length: 64\n",
      "Train: 2035 batches trained, 0 episodes: 103.37666726112366 sec\n",
      "  1395/150000 [..............................] - ETA: 84:38:59experience length: 63\n",
      "Train: 2010 batches trained, 0 episodes: 89.35331511497498 sec\n",
      "  1463/150000 [..............................] - ETA: 84:24:32experience length: 70\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
