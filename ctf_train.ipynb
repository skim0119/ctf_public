{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Notes\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "- This notebook does not include running the CtF game with the RL policy. Using the network will be separately scripted in policy/policy_RL1.py.\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.consts as CONST\n",
    "import numpy as np\n",
    "\n",
    "# the modules that you can use to generate the policy.\n",
    "import policy.patrol \n",
    "import policy.random\n",
    "import policy.simple # custon written policy\n",
    "import policy.policy_RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"cap-v0\") # initialize the environment\n",
    "policy_red = policy.random.PolicyGen(env.get_map, env.get_team_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.97\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0.0\n",
    "    for t in reversed(range(r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def discount_rewards_multiagent(r, n_agents):\n",
    "    discounted_r = np.reshape(r,(-1,n_agents)) # Reshape the r into vertical matrix with 'n_agents' columns\n",
    "    for idx in range(n_agents):\n",
    "        column = discounted_r[:,idx]                     # extract single column\n",
    "        discounted_r[:,idx] = discount_rewards(column);  # Run discount_reward on the column, and substitute\n",
    "    return np.reshape(discounted_r,(-1))                 # return the flattened matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, in_size,action_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_input = tf.placeholder(shape=in_size,dtype=tf.float32, name='state')\n",
    "        conv1 = slim.conv2d(self.state_input, 48, [3,3],# activation_fn=tf.nn.relu,\n",
    "                            scope='conv1')\n",
    "        conv2 = slim.conv2d(conv1, 96, [2,2],# activation_fn=tf.nn.relu,\n",
    "                            scope='conv2')\n",
    "        flat  = tf.reshape(conv2, [-1, 7*7*96])#slim.flatten(conv2)\n",
    "        \n",
    "        dense = slim.fully_connected(flat, action_size,\n",
    "                                            #activation_fn=tf.nn.relu,\n",
    "                                            biases_initializer=None)\n",
    "        self.output = tf.nn.softmax(dense, name='action')\n",
    "        #self.chosen_action = tf.argmax(self.output,1, name='action')\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "            self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skim0119/anaconda3/envs/ctf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "myAgent = agent(lr=1e-4,in_size=[None,7,7,6],action_size=5) #Load the agent.\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step') # global step\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100000 #Set total number of episodes to train agent on.\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "save_frequency = 100\n",
    "\n",
    "exploration_final = 0.01\n",
    "exploration = 1.0\n",
    "exploration_decay = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "# Launch the session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Setup Save and Restore Network\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('./model')\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN  = CONST.UNKNOWN # -1\n",
    "TEAM1_BG = CONST.TEAM1_BACKGROUND # 0\n",
    "TEAM2_BG = CONST.TEAM2_BACKGROUND # 1\n",
    "TEAM1_AG = CONST.TEAM1_UGV # 2\n",
    "TEAM2_AG = CONST.TEAM2_UGV # 4\n",
    "TEAM1_FL = CONST.TEAM1_FLAG # 6\n",
    "TEAM2_FL = CONST.TEAM2_FLAG # 7\n",
    "OBSTACLE = CONST.OBSTACLE # 8\n",
    "DEAD     = CONST.DEAD # 9\n",
    "SELECTED = CONST.SELECTED # 10\n",
    "COMPLETED= CONST.COMPLETED # 11\n",
    "\n",
    "VISION_RANGE = CONST.UGV_RANGE\n",
    "VISION_dX    = 2*VISION_RANGE-1\n",
    "VISION_dY    = 2*VISION_RANGE-1\n",
    "\n",
    "def one_hot_encoder(state, agents):\n",
    "    ret = np.zeros((len(agents),7,7,6))\n",
    "    # team 1 : (1), team 2 : (-1), map elements: (0)\n",
    "    map_channel = {UNKNOWN:0, DEAD:0,\n",
    "                   TEAM1_BG:1, TEAM2_BG:1,\n",
    "                   TEAM1_AG:2, TEAM2_AG:2,\n",
    "                   3:3, 5:3, # UAV, does not need to be included for now\n",
    "                   TEAM1_FL:4, TEAM2_FL:4,\n",
    "                   OBSTACLE:5}\n",
    "    map_color   = {UNKNOWN:0, DEAD:0, OBSTACLE:0,\n",
    "                   TEAM1_BG:1, TEAM2_BG:-1,\n",
    "                   TEAM1_AG:1, TEAM2_AG:-1,\n",
    "                   3:1, 5:-1, # UAV, does not need to be included for now\n",
    "                   TEAM1_FL:1, TEAM2_FL:-1}\n",
    "\n",
    "    \n",
    "    # Expand the observation with 3-thickness wall\n",
    "    # - in order to avoid dealing with the boundary\n",
    "    sx, sy = state.shape\n",
    "    _state = np.ones((sx+8, sy+8)) * 8 # 8 for obstacle\n",
    "    _state[4:4+sx, 4:4+sy] = state\n",
    "    state = _state\n",
    "\n",
    "    for idx,agent in enumerate(agents):\n",
    "        # Initialize Variables\n",
    "        x, y = agent.get_loc()\n",
    "        x += 4\n",
    "        y += 4\n",
    "        vision = state[x-3:x+4, y-3:y+4] # extract the limited view for the agent (5x5)\n",
    "        for i in range(len(vision)):\n",
    "            for j in range(len(vision[0])):\n",
    "                if vision[i][j] != -1:\n",
    "                    channel = map_channel[vision[i][j]]\n",
    "                    ret[idx][i][j][channel] = map_color[vision[i][j]]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0/100000 [.....] - ETA: 0s Last 100 Average reward :  139.4 0.01\n",
      "save:  1691\n",
      "   100/100000 [.....] - ETA: 56:50:24 Last 100 Average reward :  87.12842105263157 0.01\n",
      "save:  1791\n",
      "   191/100000 [.....] - ETA: 56:15:04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-07edec633d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_team_blue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmyAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m#Probabilistically pick an action given our network outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# divide by sum : normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ctf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ctf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/ctf/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "total_reward = []\n",
    "total_lenght = []\n",
    "\n",
    "reward_table = []\n",
    "\n",
    "action_space = 5\n",
    "n_agent = len(env.get_team_blue)\n",
    "\n",
    "gradBuffer = sess.run(tf.trainable_variables())\n",
    "for ix,grad in enumerate(gradBuffer):\n",
    "    gradBuffer[ix] = grad * 0\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,width=5)\n",
    "while i < total_episodes:\n",
    "    progbar.update(i) # update progress bar\n",
    "    s = env.reset(map_size=20, policy_red=policy_red) #, render_mode='env')\n",
    "    prev_reward=0\n",
    "    running_reward = 0\n",
    "    ep_history = []\n",
    "    for j in range(max_ep):\n",
    "        obs = one_hot_encoder(s, env.get_team_blue).tolist() # observation\n",
    "        if exploration > np.random.random(): # explorate\n",
    "            a = np.random.randint(action_space, size=len(env.get_team_blue)).tolist()\n",
    "        else:\n",
    "            obs = one_hot_encoder(s, env.get_team_blue).tolist()\n",
    "            a = sess.run(myAgent.output, feed_dict={myAgent.state_input:obs})\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a = [np.random.choice(action_space, p=a[x]/sum(a[x])) for x in range(n_agent)] # divide by sum : normalize\n",
    "        s1,r_a,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
    "        \n",
    "        # Rendering\n",
    "        #env.render(mode=\"fast\")\n",
    "        #time.sleep(0.05)\n",
    "        \n",
    "        r = r_a-prev_reward # immediate reward\n",
    "        prev_reward = r_a\n",
    "        \n",
    "        for state, act in zip(obs, a):\n",
    "            ep_history.append([state,act,r,s1])\n",
    "        s = s1\n",
    "        running_reward += r\n",
    "        if d == True or r > 0:\n",
    "            #Update the network.\n",
    "            ep_history = np.array(ep_history)\n",
    "            ep_history[:,2] = discount_rewards_multiagent(ep_history[:,2], len(env.get_team_blue))\n",
    "            #ep_history[:,2] = discount_rewards(ep_history[:,2], len(env.get_team_blue))\n",
    "            feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "                       myAgent.action_holder:ep_history[:,1],\n",
    "                       myAgent.state_input:np.stack(ep_history[:,0])}\n",
    "            grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "            for idx,grad in enumerate(grads):\n",
    "                gradBuffer[idx] += grad\n",
    "\n",
    "            if i % update_frequency == 0 and i != 0:\n",
    "                feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "\n",
    "            total_reward.append(running_reward)\n",
    "            total_lenght.append(j)\n",
    "            break\n",
    "\n",
    "    exploration = max(exploration_decay*exploration, exploration_final)\n",
    "        #Update our running tally of scores.\n",
    "    if i % save_frequency == 0:\n",
    "        print(' Last 100 Average reward : ', np.mean(total_reward[-100:]), exploration)\n",
    "        reward_table.append(np.mean(total_reward[-100:]))\n",
    "        if len(reward_table) > 100:\n",
    "            reward_table.pop(0)\n",
    "        saver.save(sess, './model/ctf_policy.ckpt', global_step=global_step)\n",
    "        print(\"save: \", sess.run(global_step))\n",
    "\n",
    "    i += 1\n",
    "    sess.run(increment_global_step_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12028f9b0>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reward_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18634942 0.18634942 0.18634942 0.18634942 0.25460228]\n",
      " [0.20451896 0.1987337  0.1987337  0.1987337  0.19927995]\n",
      " [0.19446717 0.19446717 0.19446717 0.19446717 0.2221313 ]\n",
      " [0.19598965 0.19489616 0.19489616 0.19489616 0.2193218 ]\n",
      " [0.18702151 0.18702151 0.18702151 0.18702151 0.25191396]]\n"
     ]
    }
   ],
   "source": [
    "obs = one_hot_encoder(s, env.get_team_blue)\n",
    "print(sess.run(myAgent.output, feed_dict={myAgent.state_input:obs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18634942 0.18634942 0.18634942 0.18634942 0.2546023 ]]\n",
      "[[0.20451896 0.19873369 0.19873369 0.19873369 0.19927993]]\n",
      "[[0.19446717 0.19446717 0.19446717 0.19446717 0.22213131]]\n",
      "[[0.19598964 0.19489616 0.19489616 0.19489616 0.2193218 ]]\n",
      "[[0.18702151 0.18702151 0.18702151 0.18702151 0.25191393]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(env.get_team_blue)):\n",
    "    sub_obs = np.stack([obs[i,:,:,:]])\n",
    "    print(sess.run(myAgent.output, feed_dict={myAgent.state_input:sub_obs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.get_team_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
