{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Simple Policy gradient with experience buffer.\n",
    "- The implementation network is slightly different\n",
    "    - Better code for mini-batch\n",
    "    - include self-play for red\n",
    "    - 19x19 vision\n",
    "\n",
    "### Sampling\n",
    "- [x] Mini-batch to update 'average' gradient\n",
    "- [x] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Environment Rolling\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [ ] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [x] Self-play\n",
    "- [ ] Batch Policy\n",
    "- [x] Variable Reward\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- enemy with different policies (zero, patrol)\n",
    "- stochastic interaction\n",
    "- Reward -> only 100 for completion (with small observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/B4R4_Rzero_FTW/ model/B4R4_Rzero_FTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='B4R4_Rzero_FTW'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.125 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import signal\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# the modules that you can use to generate the policy.\n",
    "import policy.patrol \n",
    "import policy.random\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import Experience_buffer, discount_rewards\n",
    "\n",
    "# Import Network\n",
    "from network.REINFORCE import REINFORCE as RF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Related\n",
    "max_ep = 150\n",
    "update_frequency = 50\n",
    "batch_size = 2000\n",
    "experience_size=10000\n",
    "\n",
    "# Saving Related\n",
    "save_network_frequency = 1000\n",
    "save_stat_frequency = 100\n",
    "moving_average_step = 100\n",
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "gamma = 0.99\n",
    "MAP_SIZE = 10\n",
    "VISION_RANGE = 9\n",
    "VISION_dX, VISION_dY = 2*VISION_RANGE+1, 2*VISION_RANGE+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red number :  4\n",
      "blue number :  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"cap-v0\") # initialize the environment\n",
    "policy_red = policy.zeros.PolicyGen(env.get_map, env.get_team_red)\n",
    "#plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "# Environment Related\n",
    "action_space = 5\n",
    "n_agent = len(env.get_team_blue)\n",
    "\n",
    "print('red number : ', len(env.get_team_red))\n",
    "print('blue number : ', len(env.get_team_blue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Criteria:\n",
    "\n",
    "- Ref. : https://arxiv.org/pdf/1807.01281.pdf\n",
    "pg(3), pg(27-28)\n",
    "\n",
    "> Since game outcome as the only reward signal is too sparse for RL to be effective, we require rewards rt to direct the learning process towards winning yet are more frequently available than the game outcome. In our approach, we operationalise the idea that each agent has a dense internal reward function (60,61,74), by specifying rt = w(ρt) based on the available game points signals ρt\n",
    "\n",
    "> w is optimised for winning probability through population based training, another level of training performed at yet a slower time scale than RL.\n",
    "\n",
    "-. Team-wise Points\n",
    "\n",
    "1. Team captured flag\n",
    "2. Team not captured flag\n",
    "3. Team captured enemy\n",
    "4. Team died by enemy\n",
    "5. Enemy captured flag\n",
    "6. Enemy not captured flag\n",
    "\n",
    "-. Individual Points\n",
    "\n",
    "7. I'm on Blue\n",
    "8. I'm on Red\n",
    "9. I moved\n",
    "10. I see flag\n",
    "11. I see enemy\n",
    "12. I see aliance\n",
    "\n",
    "-. global\n",
    "13. times up but lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_criteria = 13\n",
    "# r_CtF_scoreboard = [100, 0, 25, -25, -100, 0, 0, 0, 0, 0, 0]\n",
    "# r_signal = [1,-1,1,-1,-1,1,1,-1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_uniform_initializer(mu, std):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.lognormal(mean=mu, sigma=std, size=shape).astype(np.float32)\n",
    "        return tf.constant(out)\n",
    "    return _initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_update_freq = 50\n",
    "class reward_signal:\n",
    "    def __init__(self, lr):\n",
    "        with tf.name_scope('reward_signals'):\n",
    "            self.signals_holder = tf.placeholder(shape=[None, r_criteria], dtype = tf.float32)\n",
    "            self.winning_holder = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.reward_matrix = layers.fully_connected(self.signals_holder, r_criteria, weights_initializer=log_uniform_initializer(0.1,10.0))\n",
    "            self.reward = tf.reduce_sum(self.reward_matrix, 1, name='reward')\n",
    "\n",
    "            self.loss = -tf.reduce_mean(tf.log(self.reward)*self.winning_holder)\n",
    "            self.optimizer=tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.update = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        for var in tf.trainable_variables(scope='reward_signals'):\n",
    "            tf.summary.histogram(var.name, var)\n",
    "        \n",
    "    def signal(s0, r, a, s1, agents, env, tubl=False):\n",
    "        signals = []\n",
    "        for idx, agent in enumerate(agents):\n",
    "            signal = []\n",
    "            # Team captured flag\n",
    "            signal.append(env.blue_win)\n",
    "            # Team not captured flag\n",
    "            signal.append(not env.blue_win)\n",
    "            # Team captured enemy\n",
    "            signal.append(r > 0 and r < 100)\n",
    "            # Team died by enemy\n",
    "            signal.append(r < 0 and r > -100)\n",
    "            # Enemy captured flag\n",
    "            signal.append(env.red_win)\n",
    "            # Enemy not captured flag\n",
    "            signal.append(not env.red_win)\n",
    "            \n",
    "            # I'm on Blue BG\n",
    "            signal.append(s1[idx][1][(int)(VISION_RANGE/2)][(int)(VISION_RANGE/2)] == 0)\n",
    "            # I'm on Red BG\n",
    "            signal.append(s1[idx][1][(int)(VISION_RANGE/2)][(int)(VISION_RANGE/2)] == 1)\n",
    "            # I moved (need some more info)\n",
    "            signal.append(r != 0)\n",
    "            # I see flag\n",
    "            signal.append(np.sum(s1[idx][4]==-1) > 0)\n",
    "            # I see enemy\n",
    "            signal.append(np.sum(s1[idx][2]==-1) > 0)\n",
    "            # I see aliance\n",
    "            signal.append(np.sum(s1[idx][2]== 1) > 0)\n",
    "            \n",
    "            # times up but lost\n",
    "            signal.append(tubl)\n",
    "            \n",
    "            signals.append(signal[:])\n",
    "        return np.array(signals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "myAgent = RF(lr=LEARNING_RATE,in_size=[None,VISION_dX,VISION_dY,6],action_size=5,grad_clip_norm=0,trainable=True)\n",
    "rs = reward_signal(lr=1e-2)\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step') # global step\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#sess = tf.Session()\n",
    "\n",
    "ma_reward = MA(moving_average_step)\n",
    "ma_length = MA(moving_average_step)\n",
    "ma_captured = MA(reward_update_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "# Setup Save and Restore Network\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(summary_):\n",
    "    with tf.device('/cpu:0'): \n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='Records/mean_reward', simple_value=ma_reward())\n",
    "        summary.value.add(tag='Records/mean_length', simple_value=ma_length())\n",
    "        summary.value.add(tag='Records/mean_succeed', simple_value=ma_captured())\n",
    "        writer.add_summary(summary, sess.run(global_step))\n",
    "        \n",
    "        #summary_str = sess.run(merged,feed_dict={myAgent.state_input:obs})\n",
    "        writer.add_summary(summary_, sess.run(global_step))\n",
    "        \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(signum, frame):\n",
    "    raise Exception('Action took too much time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_rollout(PARTIAL=False):\n",
    "    # Run single episode, return the results\n",
    "    # Temporary fix for episode reset\n",
    "    \n",
    "    s = env.reset(map_size=MAP_SIZE, policy_red=policy_red)\n",
    "        \n",
    "    if PARTIAL:\n",
    "        obs_next = one_hot_encoder(s, env.get_team_blue) # partial observation\n",
    "    else:\n",
    "        obs_next = one_hot_encoder(env._env, env.get_team_blue, VISION_RANGE) # Full observation\n",
    "    \n",
    "    ep_history = []\n",
    "    indv_history = [[] for _ in range(len(env.get_team_blue))]\n",
    "    \n",
    "    was_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "    prev_reward=0\n",
    "    frame=0\n",
    "    for frame in range(max_ep+1):\n",
    "        obs = obs_next\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            act_prob = sess.run(myAgent.output, feed_dict={myAgent.state_input:obs})\n",
    "        act = [np.random.choice(action_space, p=act_prob[x]/sum(act_prob[x])) for x in range(n_agent)] # divide by sum : normalize\n",
    "            \n",
    "        s,r1,d,_ = env.step(act) #Get our reward for taking an action given a bandit.\n",
    "        if PARTIAL:\n",
    "            obs_next = one_hot_encoder(s, env.get_team_blue) # partial observation\n",
    "        else:\n",
    "            obs_next = one_hot_encoder(env._env, env.get_team_blue, VISION_RANGE) # Full observation\n",
    "        \n",
    "        rr = r1-prev_reward\n",
    "\n",
    "        if frame == max_ep and d == False:\n",
    "            #r -= frame * (30/1000)\n",
    "            #r = -100\n",
    "            r1 = -100\n",
    "            d = True\n",
    "        r_signals = reward_signal.signal(obs, rr, act, obs_next, env.get_team_blue, env, frame==max_ep and not d)\n",
    "        r = sess.run(rs.reward, feed_dict={rs.signals_holder:r_signals})\n",
    "        \n",
    "        # Push history for individual that 'was' alive previous frame\n",
    "        for idx, agent in enumerate(env.get_team_blue):\n",
    "            if was_alive[idx]:\n",
    "                indv_history[idx].append([obs[idx],act[idx],r[idx],r_signals[idx]])\n",
    "        \n",
    "        # State Transition\n",
    "        prev_reward = r1\n",
    "        was_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "\n",
    "    for idx, history in enumerate(indv_history):\n",
    "        if len(history)==0: continue\n",
    "        _history = np.array(history)\n",
    "        _history[:,2] = discount_rewards(_history[:,2], gamma)\n",
    "        ep_history.extend(_history)\n",
    "            \n",
    "    if len(ep_history) > 0:        \n",
    "        ep_history = np.stack(ep_history)\n",
    "    \n",
    "    return [frame, ep_history, r1, env.blue_win]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_training(num_ep):\n",
    "    ep = sess.run(global_step)\n",
    "\n",
    "    exp_buffer = Experience_buffer(experience_shape=4)\n",
    "    try:\n",
    "        progbar = tf.keras.utils.Progbar(num_ep,width=5, interval=0.5)\n",
    "        for i in range(num_ep):\n",
    "            progbar.update(i) # update progress bar\n",
    "            ep += 1\n",
    "            # Run episode\n",
    "            frame, history, reward, did_won = policy_rollout(True)\n",
    "\n",
    "            # Add history\n",
    "            exp_buffer.add(history)\n",
    "\n",
    "            batch_history = exp_buffer.sample(batch_size) # Sample from experience replay\n",
    "            if len(batch_history) > 0:\n",
    "                feed_dict={myAgent.reward_holder:batch_history[:,2],\n",
    "                           myAgent.action_holder:batch_history[:,1],\n",
    "                           myAgent.state_input:np.stack(batch_history[:,0])}\n",
    "                with tf.device('/gpu:0'):\n",
    "                    sess.run(myAgent.accumulate_gradient, feed_dict=feed_dict)\n",
    "\n",
    "            if ep % update_frequency == 0 and ep != 0:\n",
    "                with tf.device('/gpu:0'):\n",
    "                    sess.run(myAgent.update_batch)\n",
    "                    sess.run(myAgent.clear_batch)\n",
    "                exp_buffer.flush()\n",
    "                                \n",
    "            # summarize and record\n",
    "            ma_reward.append(reward)\n",
    "            ma_length.append(frame)\n",
    "            ma_captured.append(env.blue_win)\n",
    "\n",
    "            if ep % reward_update_freq == 0 and ep != 0:\n",
    "                # update reward signal weights\n",
    "                feed_dict_rs = {rs.signals_holder : np.stack(batch_history[:,3]),\n",
    "                                rs.winning_holder : np.ones(len(batch_history[:,3]))*ma_captured()}\n",
    "                sess.run(rs.update, feed_dict=feed_dict_rs)\n",
    "                \n",
    "            if ep % save_stat_frequency == 0 and ep != 0:\n",
    "                summary_ = sess.run(merged, feed_dict={**feed_dict, **feed_dict_rs})\n",
    "                record(summary_)\n",
    "\n",
    "            # save weight\n",
    "            if ep % save_network_frequency == 0:\n",
    "                saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_step)\n",
    "\n",
    "            sess.run(increment_global_step_op)\n",
    "        return 0\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n\\nManually stopped the training (KeyboardInterrupt)');\n",
    "        saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_step)\n",
    "        print(\"save: \", sess.run(global_step), 'episodes')\n",
    "        \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_zero_training=500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with fixed policy\n",
      "   522/500000 [.....] - ETA: 126:57:05"
     ]
    }
   ],
   "source": [
    "print('Training with fixed policy')\n",
    "policy_red = policy.zeros.PolicyGen(env.get_map, env.get_team_red)\n",
    "run_training(initial_zero_training)\n",
    "print('training with fixed red: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
