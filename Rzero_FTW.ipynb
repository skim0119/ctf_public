{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Simple Policy gradient with experience buffer.\n",
    "- The implementation network is slightly different\n",
    "    - Better code for mini-batch\n",
    "    - include self-play for red\n",
    "    - 19x19 vision\n",
    "\n",
    "### Sampling\n",
    "- [x] Mini-batch to update 'average' gradient\n",
    "- [x] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Environment Rolling\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [ ] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [x] Self-play\n",
    "- [ ] Batch Policy\n",
    "- [x] Variable Reward\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- enemy with different policies (zero, patrol)\n",
    "- stochastic interaction\n",
    "- Reward -> only 100 for completion (with small observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/B4R4_Rzero_FTW/ model/B4R4_Rzero_FTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='B4R4_Rzero_FTW'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.125 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import signal\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# the modules that you can use to generate the policy.\n",
    "import policy.patrol \n",
    "import policy.random\n",
    "import policy.simple # custon written policy\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import Experience_buffer, discount_rewards\n",
    "\n",
    "# Import Network\n",
    "from network import REINFORCE as RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Related\n",
    "max_ep = 150\n",
    "update_frequency = 50\n",
    "batch_size = 2000\n",
    "experience_size=10000\n",
    "\n",
    "# Saving Related\n",
    "save_network_frequency = 1000\n",
    "save_stat_frequency = 100\n",
    "moving_average_step = 100\n",
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "gamma = 0.99\n",
    "MAP_SIZE = 20\n",
    "VISION_RANGE = 9\n",
    "VISION_dX, VISION_dY = 2*VISION_RANGE+1, 2*VISION_RANGE+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red number :  4\n",
      "blue number :  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"cap-v0\") # initialize the environment\n",
    "policy_red = policy.zeros.PolicyGen(env.get_map, env.get_team_red)\n",
    "#plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "# Environment Related\n",
    "action_space = 5\n",
    "n_agent = len(env.get_team_blue)\n",
    "\n",
    "print('red number : ', len(env.get_team_red))\n",
    "print('blue number : ', len(env.get_team_blue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Criteria:\n",
    "\n",
    "- Ref. : https://arxiv.org/pdf/1807.01281.pdf\n",
    "pg(3), pg(27-28)\n",
    "\n",
    "> Since game outcome as the only reward signal is too sparse for RL to be effective, we require rewards rt to direct the learning process towards winning yet are more frequently available than the game outcome. In our approach, we operationalise the idea that each agent has a dense internal reward function (60,61,74), by specifying rt = w(ρt) based on the available game points signals ρt\n",
    "\n",
    "> w is optimised for winning probability through population based training, another level of training performed at yet a slower time scale than RL.\n",
    "\n",
    "-. Team-wise Points\n",
    "\n",
    "1. Team captured flag\n",
    "2. Team not captured flag\n",
    "3. Team captured enemy\n",
    "4. Team died by enemy\n",
    "5. Enemy captured flag\n",
    "6. Enemy not captured flag\n",
    "\n",
    "-. Individual Points\n",
    "\n",
    "7. I'm on Blue\n",
    "8. I'm on Red\n",
    "9. I moved\n",
    "10. I see flag\n",
    "11. I see enemy\n",
    "12. I see aliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_criteria = 12\n",
    "r_CtF_scoreboard = [100, 0, 25, -25, -100, 0, 0, 0, 0, 0, 0]\n",
    "r_CtF_scoreboard = [100, 0, 25, -25, -100, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reward_signal:\n",
    "    def __init__(self, lr = ):\n",
    "        with tf.name_scope('reward_matrix'):\n",
    "            self.performance_chart = tf.placeholder(shape=[None, r_criteria], dtype = tf.float32)\n",
    "            self.reward_matrix = tf.Variable\n",
    "            self.reward_matrix = layers.fully_connected(self.performance_chart, r_criteria)\n",
    "            self.reward_holder = tf.reduce_sum(self.reward_matrix, name='reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "reward_signal =\n",
    "myAgent = RF(lr=LEARNING_RATE,in_size=[None,VISION_dX,VISION_dY,6],action_size=5,grad_clip_norm=0,trainable=True)\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step') # global step\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/B4R4_self_VANILLA/ctf_policy.ckpt-50507\n",
      "Load Model :  ./model/B4R4_self_VANILLA/ctf_policy.ckpt-50507\n"
     ]
    }
   ],
   "source": [
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#sess = tf.Session()\n",
    "\n",
    "ma_reward = MA(moving_average_step)\n",
    "ma_length = MA(moving_average_step)\n",
    "ma_captured = MA(moving_average_step)\n",
    "\n",
    "# Setup Save and Restore Network\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(summary_):\n",
    "    with tf.device('/cpu:0'): \n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='Records/mean_reward', simple_value=ma_reward())\n",
    "        summary.value.add(tag='Records/mean_length', simple_value=ma_length())\n",
    "        summary.value.add(tag='Records/mean_succeed', simple_value=ma_captured())\n",
    "        writer.add_summary(summary, sess.run(global_step))\n",
    "        \n",
    "        #summary_str = sess.run(merged,feed_dict={myAgent.state_input:obs})\n",
    "        writer.add_summary(summary_, sess.run(global_step))\n",
    "        \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(signum, frame):\n",
    "    print('Reset Taking Too Long')\n",
    "    raise Exception('Action took too much time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_rollout(PARTIAL=False):\n",
    "    # Run single episode, return the results\n",
    "    # Temporary fix for episode reset\n",
    "    flag = True\n",
    "    while flag:\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(3) #Set the parameter to the amount of seconds you want to wait\n",
    "        try:\n",
    "            s = env.reset(map_size=MAP_SIZE, policy_red=policy_red)\n",
    "            flag = False\n",
    "        except:\n",
    "            print('timeout. retry:')\n",
    "            flag = True\n",
    "        signal.alarm(0) #Disables the alarm\n",
    "        \n",
    "    if PARTIAL:\n",
    "        obs_next = one_hot_encoder(s, env.get_team_blue) # partial observation\n",
    "    else:\n",
    "        obs_next = one_hot_encoder(env._env, env.get_team_blue, VISION_RANGE) # Full observation\n",
    "    \n",
    "    ep_history = []\n",
    "    indv_history = [[] for _ in range(len(env.get_team_blue))]\n",
    "    \n",
    "    was_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "    prev_reward=0\n",
    "    frame=0\n",
    "    for frame in range(max_ep+1):\n",
    "        obs = obs_next\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            act_prob = sess.run(myAgent.output, feed_dict={myAgent.state_input:obs})\n",
    "        act = [np.random.choice(action_space, p=act_prob[x]/sum(act_prob[x])) for x in range(n_agent)] # divide by sum : normalize\n",
    "            \n",
    "        s,r1,d,_ = env.step(act) #Get our reward for taking an action given a bandit.\n",
    "\n",
    "        r = r1-prev_reward\n",
    "\n",
    "        if frame == max_ep and d == False:\n",
    "            #r -= frame * (30/1000)\n",
    "            r = -100\n",
    "            r1 = -100\n",
    "\n",
    "        if PARTIAL:\n",
    "            obs_next = one_hot_encoder(s, env.get_team_blue) # partial observation\n",
    "        else:\n",
    "            obs_next = one_hot_encoder(env._env, env.get_team_blue, VISION_RANGE) # Full observation\n",
    "        \n",
    "        # Push history for individual that 'was' alive previous frame\n",
    "        for idx, agent in enumerate(env.get_team_blue):\n",
    "            if was_alive[idx]:\n",
    "                indv_history[idx].append([obs[idx],act[idx],r])\n",
    "        \n",
    "        # State Transition\n",
    "        prev_reward = r1\n",
    "        was_alive = [ag.isAlive for ag in env.get_team_blue]\n",
    "        \n",
    "        if d:\n",
    "            break\n",
    "\n",
    "    for idx, history in enumerate(indv_history):\n",
    "        if len(history)==0: continue\n",
    "        _history = np.array(history)\n",
    "        _history[:,2] = discount_rewards(_history[:,2], gamma)\n",
    "        ep_history.extend(_history)\n",
    "            \n",
    "    if len(ep_history) > 0:        \n",
    "        ep_history = np.stack(ep_history)\n",
    "    \n",
    "    return [frame, ep_history, r1, env.blue_win, obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_training(num_ep):\n",
    "    ep = sess.run(global_step)\n",
    "\n",
    "    exp_buffer = Experience_buffer(experience_shape=3)\n",
    "    try:\n",
    "        progbar = tf.keras.utils.Progbar(num_ep,width=5, interval=0.5)\n",
    "        for i in range(num_ep):\n",
    "            progbar.update(i) # update progress bar\n",
    "            ep += 1\n",
    "            # Run episode\n",
    "            frame, history, reward, did_won, obs = policy_rollout(True)\n",
    "\n",
    "            # Add history\n",
    "            exp_buffer.add(history)\n",
    "\n",
    "            batch_history = exp_buffer.sample(batch_size) # Sample from experience replay\n",
    "            if len(batch_history) > 0:\n",
    "                feed_dict={myAgent.reward_holder:batch_history[:,2],\n",
    "                           myAgent.action_holder:batch_history[:,1],\n",
    "                           myAgent.state_input:np.stack(batch_history[:,0])}\n",
    "                with tf.device('/gpu:0'):\n",
    "                    sess.run(myAgent.accumulate_gradient, feed_dict=feed_dict)\n",
    "\n",
    "            if ep % update_frequency == 0 and ep != 0:\n",
    "                with tf.device('/gpu:0'):\n",
    "                    sess.run(myAgent.update_batch)\n",
    "                    sess.run(myAgent.clear_batch)\n",
    "                exp_buffer.flush()\n",
    "                                \n",
    "            # summarize and record\n",
    "            ma_reward.append(reward)\n",
    "            ma_length.append(frame)\n",
    "            ma_captured.append(env.blue_win)\n",
    "\n",
    "            if ep % save_stat_frequency == 0 and ep != 0:\n",
    "                summary_ = sess.run(merged, feed_dict=feed_dict)\n",
    "                record(summary_)\n",
    "\n",
    "            # save weight\n",
    "            if ep % save_network_frequency == 0:\n",
    "                saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_step)\n",
    "\n",
    "            sess.run(increment_global_step_op)\n",
    "        return 0\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n\\nManually stopped the training (KeyboardInterrupt)');\n",
    "        saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_step)\n",
    "        print(\"save: \", sess.run(global_step), 'episodes')\n",
    "        \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Play Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_zero_training=0\n",
    "weight_change_freq = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with fixed policy\n",
      "training with fixed red: Done\n"
     ]
    }
   ],
   "source": [
    "print('Training with fixed policy')\n",
    "policy_red = policy.zeros.PolicyGen(env.get_map, env.get_team_red)\n",
    "run_training(initial_zero_training)\n",
    "print('training with fixed red: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exist\n"
     ]
    }
   ],
   "source": [
    "policy_red = policy.policy_RL.PolicyGen(env.get_map, env.get_team_red,\n",
    "                                        model_dir=MODEL_PATH, color='red')\n",
    "\n",
    "while True:\n",
    "    if run_training(weight_change_freq): break\n",
    "    print('training at : ', sess.run(global_step), '  red policy updated')\n",
    "    if sess.run(global_step) % weight_change_freq == 0:\n",
    "        policy_red.reset_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
